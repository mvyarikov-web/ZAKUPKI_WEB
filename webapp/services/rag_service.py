"""–û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å RAG –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é, –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.
"""
import os
import json
import hashlib
from typing import List, Dict, Any, Optional, Tuple
from flask import current_app
import openai

from webapp.models.rag_models import RAGDatabase
from webapp.services.chunking import chunk_document, TextChunker
from webapp.services.embeddings import get_embeddings_service
from document_processor.core import DocumentProcessor
from utils.api_keys_manager_multiple import get_api_keys_manager_multiple


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è RAG-–∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
    
    def __init__(
        self,
        database_url: Optional[str] = None,
        api_key: Optional[str] = None
    ):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG-—Å–µ—Ä–≤–∏—Å–∞.
        
        Args:
            database_url: URL –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
            api_key: API –∫–ª—é—á OpenAI
        """
        self.database_url = database_url or self._get_database_url()
        self.api_key = api_key or self._get_api_key()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        try:
            self.db = RAGDatabase(self.database_url)
            self.db_available = True
        except Exception as e:
            try:
                current_app.logger.warning(f'PostgreSQL –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}')
            except Exception:
                pass
            self.db = None
            self.db_available = False
        
        self.embeddings_service = get_embeddings_service(api_key=self.api_key)
        self.doc_processor = DocumentProcessor()
    
    def _get_database_url(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å URL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        try:
            return current_app.config.get('DATABASE_URL', '')
        except Exception:
            return os.environ.get('DATABASE_URL', '')
    
    def _get_api_key(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å API –∫–ª—é—á OpenAI."""
        api_key = os.environ.get('OPENAI_API_KEY', '')
        if api_key:
            return api_key
        
        try:
            return current_app.config.get('OPENAI_API_KEY', '')
        except Exception:
            return ''
    
    def initialize_database(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ö–µ–º—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""
        if not self.db_available:
            raise RuntimeError("–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
        
        self.db.initialize_schema()
    
    def index_document(
        self,
        file_path: str,
        upload_folder: str
    ) -> Tuple[bool, str, Optional[Dict[str, Any]]]:
        """
        –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –¥–ª—è RAG.
        
        Args:
            file_path: –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
            upload_folder: –ë–∞–∑–æ–≤–∞—è –ø–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏
            
        Returns:
            Tuple (success, message, stats)
        """
        if not self.db_available:
            return False, "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", None
        
        if not self.api_key:
            return False, "OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω", None
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å
            full_path = os.path.join(upload_folder, file_path)
            
            if not os.path.exists(full_path):
                return False, f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}", None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = self.doc_processor.extract_text(full_path)
            
            if not text or not text.strip():
                return False, f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ñ–∞–π–ª–∞", None
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞
            file_hash = self._calculate_file_hash(full_path)
            file_size = os.path.getsize(full_path)
            file_name = os.path.basename(file_path)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω –ª–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç —Å —Ç–∞–∫–∏–º –∂–µ —Ö–µ—à–µ–º
            existing_doc = self.db.get_document_by_path(file_path)
            if existing_doc and existing_doc.get('file_hash') == file_hash:
                return True, f"–î–æ–∫—É–º–µ–Ω—Ç —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω", {
                    'document_id': existing_doc['id'],
                    'chunks_count': 0,
                    'skipped': True
                }
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –±–∞–∑—É
            doc_id = self.db.add_document(
                file_path=file_path,
                file_name=file_name,
                file_hash=file_hash,
                file_size=file_size,
                metadata={'source': 'upload'}
            )
            
            # –ß–∞–Ω–∫—É–µ–º —Ç–µ–∫—Å—Ç
            chunk_size = self._get_config('RAG_CHUNK_SIZE', 2000)
            overlap = self._get_config('RAG_CHUNK_OVERLAP', 3)
            
            chunks = chunk_document(
                text,
                file_path=file_path,
                chunk_size_tokens=chunk_size,
                overlap_sentences=overlap
            )
            
            if not chunks:
                return False, "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —á–∞–Ω–∫–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞", None
            
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤
            chunk_texts = [c['content'] for c in chunks]
            embeddings = self.embeddings_service.get_embeddings_batch(chunk_texts)
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
            chunks_with_embeddings = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                if embedding is None:
                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∞–Ω–∫–∏ –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                
                chunks_with_embeddings.append({
                    'document_id': doc_id,
                    'chunk_index': i,
                    'content': chunk['content'],
                    'content_hash': chunk['content_hash'],
                    'embedding': embedding,
                    'token_count': chunk['token_count'],
                    'metadata': chunk.get('metadata', {})
                })
            
            if not chunks_with_embeddings:
                return False, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —á–∞–Ω–∫–æ–≤", None
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —á–∞–Ω–∫–∏ –≤ –±–∞–∑—É
            self.db.add_chunks(chunks_with_embeddings)
            
            return True, f"–î–æ–∫—É–º–µ–Ω—Ç –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ", {
                'document_id': doc_id,
                'chunks_count': len(chunks_with_embeddings),
                'skipped': False
            }
        
        except Exception as e:
            try:
                current_app.logger.exception(f'–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞ {file_path}: {e}')
            except Exception:
                pass
            return False, f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {str(e)}", None
    
    def search_and_analyze(
        self,
        query: str,
        file_paths: List[str],
        model: str = "gpt-4o-mini",
        top_k: int = 5,
        max_output_tokens: int = 600,
        temperature: float = 0.3,
        upload_folder: Optional[str] = None,
        search_params: Optional[Dict[str, Any]] = None
    ) -> Tuple[bool, str, Optional[Dict[str, Any]]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å RAG-–∞–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å/–ø—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            model: –ú–æ–¥–µ–ª—å GPT –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            max_output_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            upload_folder: –ü–∞–ø–∫–∞ —Å —Ñ–∞–π–ª–∞–º–∏ (–¥–ª—è —Ñ–æ–ª–±—ç–∫–∞)
            search_params: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è Perplexity (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º –ø–æ–∏—Å–∫–∞)
            
        Returns:
            Tuple (success, message, result_dict)
        """
        if not self.db_available:
            return False, "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", None
        
        if not self.api_key:
            return False, "OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω", None
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            try:
                query_embedding = self.embeddings_service.get_embedding(query)
            except Exception as emb_err:
                current_app.logger.exception(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞: {emb_err}')
                return False, f"–û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {str(emb_err)}", None
            
            if not query_embedding:
                return False, "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API-–∫–ª—é—á –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ OpenAI.", None
            
            # –ü–æ–ª—É—á–∞–µ–º ID –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            document_ids = []
            try:
                for file_path in file_paths:
                    doc = self.db.get_document_by_path(file_path)
                    if doc:
                        document_ids.append(doc['id'])
            except Exception as db_err:
                # –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î - –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—É—é
                current_app.logger.warning(f'–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î: {db_err}')
                self.db_available = False
                return False, "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", None
            
            if not document_ids:
                return False, "–î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã", None
            
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏
            min_similarity = self._get_config('RAG_MIN_SIMILARITY', 0.7)
            
            try:
                relevant_chunks = self.db.search_similar_chunks(
                    query_embedding=query_embedding,
                    top_k=top_k,
                    min_similarity=min_similarity,
                    document_ids=document_ids
                )
            except Exception as db_err:
                # –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ - –ø–æ–º–µ—á–∞–µ–º –ë–î –∫–∞–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—É—é
                current_app.logger.warning(f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î: {db_err}')
                self.db_available = False
                return False, "–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞", None
            
            if not relevant_chunks:
                return False, "–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", None
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤
            context = self._build_context(relevant_chunks)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            system_prompt = self._get_system_prompt()
            user_prompt = self._build_user_prompt(query, context)
            
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
            chunker = TextChunker()
            input_tokens = chunker.count_tokens(system_prompt + user_prompt)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ –º–æ–¥–µ–ª–∏ (OpenAI –∏–ª–∏ DeepSeek)
            client = self._get_client_for_model(model)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            request_params = {
                "model": model,
                "messages": [
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                "temperature": temperature,
                "response_format": {"type": "json_object"}
            }
            
            # –ü—Ä–∏ –≤–∫–ª—é—á—ë–Ω–Ω–æ–º –ø–æ–∏—Å–∫–µ –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º max_tokens, –∏–Ω–∞—á–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç
            if not search_params:
                request_params["max_tokens"] = max_output_tokens
            # else: max_tokens –Ω–µ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º, —á—Ç–æ–±—ã –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å –æ—Ç–≤–µ—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ
            
            # –í–ê–†–ò–ê–ù–¢ B: –Ø–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–º —á–µ—Ä–µ–∑ disable_search
            # –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω—ã –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ (–¥–ª—è Perplexity models —Å –ø–æ–∏—Å–∫–æ–º)
            if search_params:
                # –†–µ–∂–∏–º –° –ü–û–ò–°–ö–û–ú: disable_search –ù–ï —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é False)
                # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞ –∫ –∑–∞–ø—Ä–æ—Å—É
                if search_params.get('max_results'):
                    request_params['max_results'] = search_params['max_results']
                if search_params.get('search_domain_filter'):
                    request_params['search_domain_filter'] = search_params['search_domain_filter']
                if search_params.get('search_recency_filter'):
                    request_params['search_recency_filter'] = search_params['search_recency_filter']
                if search_params.get('search_after_date'):
                    request_params['search_after_date'] = search_params['search_after_date']
                if search_params.get('search_before_date'):
                    request_params['search_before_date'] = search_params['search_before_date']
                if search_params.get('country'):
                    request_params['country'] = search_params['country']
                if search_params.get('max_tokens_per_page'):
                    request_params['max_tokens_per_page'] = search_params['max_tokens_per_page']
                
                current_app.logger.info(f'üåê –†–µ–∂–∏–º –° –ü–û–ò–°–ö–û–ú: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã = {search_params}')
            else:
                # –†–µ–∂–∏–º –ë–ï–ó –ü–û–ò–°–ö–ê: —è–≤–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º –ø–æ–∏—Å–∫ –¥–ª—è Perplexity –º–æ–¥–µ–ª–µ–π
                if 'sonar' in model.lower() or 'perplexity' in model.lower():
                    request_params['disable_search'] = True
                    current_app.logger.info(f'üö´ –†–µ–∂–∏–º –ë–ï–ó –ü–û–ò–°–ö–ê: disable_search = True –¥–ª—è –º–æ–¥–µ–ª–∏ {model}')
            
            response = client.chat.completions.create(**request_params)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–∫—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞ –≤ –æ—Ç–≤–µ—Ç–µ
            search_used = False
            if hasattr(response, 'search_results') and response.search_results:
                search_used = True
                current_app.logger.info(f'‚úÖ –ü–æ–∏—Å–∫ –ë–´–õ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(response.search_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
            else:
                current_app.logger.info(f'üìù –ü–æ–∏—Å–∫ –ù–ï –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω (—Ç–æ–ª—å–∫–æ –∑–Ω–∞–Ω–∏—è –º–æ–¥–µ–ª–∏)')
            
            if not response or not response.choices:
                return False, "–ù–µ –ø–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç GPT", None
            
            # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç
            response_text = response.choices[0].message.content
            
            try:
                structured_response = json.loads(response_text)
            except json.JSONDecodeError:
                return False, "–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–∞ GPT", None
            
            # –ü–æ–¥—Å—á—ë—Ç —Ç–æ–∫–µ–Ω–æ–≤
            usage = response.usage
            actual_input_tokens = usage.prompt_tokens if usage else input_tokens
            actual_output_tokens = usage.completion_tokens if usage else 0
            total_tokens = usage.total_tokens if usage else (input_tokens + actual_output_tokens)
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            processed_result = self._postprocess_result(
                structured_response,
                relevant_chunks
            )
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –±—ã–ª –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ø–æ–∏—Å–∫ (–ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞)
            search_was_used = hasattr(response, 'search_results') and response.search_results is not None and len(response.search_results) > 0
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å —Å—É—Ñ—Ñ–∏–∫—Å–æ–º "+ Search" –µ—Å–ª–∏ –ø–æ–∏—Å–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω
            model_display_name = model
            if search_was_used:
                model_display_name = f"{model} + Search"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                'summary': processed_result.get('summary', []),
                'equipment': processed_result.get('equipment', []),
                'installation': processed_result.get('installation', {}),
                'usage': {
                    'input_tokens': actual_input_tokens,
                    'output_tokens': actual_output_tokens,
                    'total_tokens': total_tokens
                },
                'model': model_display_name,
                'search_used': search_was_used,  # –§–ª–∞–≥ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ–∏—Å–∫–∞
                'chunks_used': len(relevant_chunks),
                'sources': [
                    {
                        'file_name': c['file_name'],
                        'similarity': c['similarity']
                    }
                    for c in relevant_chunks
                ]
            }
            
            return True, "–ê–Ω–∞–ª–∏–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ", result
        
        except Exception as e:
            try:
                current_app.logger.exception(f'–û—à–∏–±–∫–∞ RAG-–∞–Ω–∞–ª–∏–∑–∞: {e}')
            except Exception:
                pass
            return False, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}", None
    
    def _build_context(self, chunks: List[Dict[str, Any]]) -> str:
        """–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ —á–∞–Ω–∫–æ–≤."""
        context_parts = []
        
        for i, chunk in enumerate(chunks):
            file_name = chunk.get('file_name', 'Unknown')
            content = chunk.get('content', '')
            similarity = chunk.get('similarity', 0)
            
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1}: {file_name}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {similarity:.2f}]\n{content}\n"
            )
        
        return "\n".join(context_parts)
    
    def _get_system_prompt(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        return """–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–∫—É–ø–æ–∫ –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Ç–µ—Ö–Ω–∏–∫–∏.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –¥–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON.

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
{
  "summary": ["–∫—Ä–∞—Ç–∫–∏–π –ø—É–Ω–∫—Ç 1", "–∫—Ä–∞—Ç–∫–∏–π –ø—É–Ω–∫—Ç 2", ... –¥–æ 10 –ø—É–Ω–∫—Ç–æ–≤],
  "equipment": [
    {
      "name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è",
      "model": "–º–æ–¥–µ–ª—å (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞)",
      "characteristics": {
        "–º–æ—â–Ω–æ—Å—Ç—å": "–∑–Ω–∞—á–µ–Ω–∏–µ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏",
        "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å": "–∑–Ω–∞—á–µ–Ω–∏–µ",
        ...
      },
      "qty": —á–∏—Å–ª–æ –∏–ª–∏ null,
      "unit": "—à—Ç/–∫–æ–º–ø–ª–µ–∫—Ç/—Ç –∏ —Ç.–ø." –∏–ª–∏ null,
      "evidence": ["–∫–æ—Ä–æ—Ç–∫–∞—è —Ü–∏—Ç–∞—Ç–∞ 1", "–∫–æ—Ä–æ—Ç–∫–∞—è —Ü–∏—Ç–∞—Ç–∞ 2"]
    }
  ],
  "installation": {
    "verdict": true|false|"unknown",
    "evidence": ["—Ü–∏—Ç–∞—Ç–∞ —Å —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ–º –º–æ–Ω—Ç–∞–∂–∞", "—Ü–∏—Ç–∞—Ç–∞ –æ –ü–ù–†", ...]
  }
}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. summary ‚Äî –∫—Ä–∞—Ç–∫–∞—è –≤—ã–∂–∏–º–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –∑–∞–∫—É–ø–∫–∏ (–¥–æ 10 –ø—É–Ω–∫—Ç–æ–≤).
2. equipment ‚Äî —Å–ø–∏—Å–æ–∫ –≤—Å–µ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è —Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏ –∏ —Ü–∏—Ç–∞—Ç–∞–º–∏-–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏.
3. installation.verdict:
   - true, –µ—Å–ª–∏ —è–≤–Ω–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –º–æ–Ω—Ç–∞–∂, —É—Å—Ç–∞–Ω–æ–≤–∫–∞, –ü–ù–†, —à–µ—Ñ-–º–æ–Ω—Ç–∞–∂, –°–ú–†, –ø—É—Å–∫–æ–Ω–∞–ª–∞–¥–∫–∞
   - false, –µ—Å–ª–∏ —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–æ "–±–µ–∑ –º–æ–Ω—Ç–∞–∂–∞" –∏–ª–∏ "—Ç–æ–ª—å–∫–æ –ø–æ—Å—Ç–∞–≤–∫–∞"
   - "unknown", –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–∞
4. evidence ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–µ —Ü–∏—Ç–∞—Ç—ã (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞—é—â–∏–µ –≤—ã–≤–æ–¥—ã.

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
    
    def _build_user_prompt(self, query: str, context: str) -> str:
        """–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç."""
        return f"""–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {query}

–§—Ä–∞–≥–º–µ–Ω—Ç—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:
{context}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON —Å–æ–≥–ª–∞—Å–Ω–æ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º."""
    
    def _postprocess_result(
        self,
        response: Dict[str, Any],
        chunks: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è, –∞–≥—Ä–µ–≥–∞—Ü–∏—è.
        
        Args:
            response: –û—Ç–≤–µ—Ç –æ—Ç GPT
            chunks: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
            
        Returns:
            –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        """
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è
        equipment_dict = {}
        for eq in response.get('equipment', []):
            key = (eq.get('name', '').lower().strip(), eq.get('model', '').lower().strip())
            
            if key in equipment_dict:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
                existing = equipment_dict[key]
                existing_chars = existing.get('characteristics', {})
                new_chars = eq.get('characteristics', {})
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –Ω–µ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ
                for k, v in new_chars.items():
                    if k not in existing_chars:
                        existing_chars[k] = v
                
                existing['characteristics'] = existing_chars
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º evidence
                existing_evidence = set(existing.get('evidence', []))
                new_evidence = set(eq.get('evidence', []))
                existing['evidence'] = list(existing_evidence | new_evidence)
            else:
                equipment_dict[key] = eq
        
        deduplicated_equipment = list(equipment_dict.values())
        
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –≤–µ—Ä–¥–∏–∫—Ç–∞ –ø–æ –º–æ–Ω—Ç–∞–∂—É
        installation = response.get('installation', {})
        verdict = installation.get('verdict', 'unknown')
        
        # –ï—Å–ª–∏ —Ö–æ—Ç—å –≥–¥–µ-—Ç–æ —É–≤–µ—Ä–µ–Ω–Ω–æ–µ "–¥–∞", —Ç–æ true
        # –ï—Å–ª–∏ –≤–µ–∑–¥–µ "–Ω–µ—Ç", —Ç–æ false
        # –ò–Ω–∞—á–µ unknown
        
        result = {
            'summary': response.get('summary', [])[:10],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 10 –ø—É–Ω–∫—Ç–∞–º–∏
            'equipment': deduplicated_equipment,
            'installation': installation
        }
        
        return result
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """–í—ã—á–∏—Å–ª–∏—Ç—å —Ö–µ—à —Ñ–∞–π–ª–∞."""
        hasher = hashlib.md5()
        
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    def _get_config(self, key: str, default: Any) -> Any:
        """–ü–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏."""
        try:
            return current_app.config.get(key, default)
        except Exception:
            return default
    
    def get_database_stats(self) -> Optional[Dict[str, int]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
        if not self.db_available:
            return None
        
        try:
            return self.db.get_stats()
        except Exception:
            return None
    
    def _get_client_for_model(self, model: str) -> openai.OpenAI:
        """
        –ü–æ–ª—É—á–∏—Ç—å OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
        
        Args:
            model: ID –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'gpt-4o-mini' –∏–ª–∏ 'deepseek-chat')
            
        Returns:
            –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –∫–ª–∏–µ–Ω—Ç OpenAI
        """
        api_keys_mgr = get_api_keys_manager_multiple()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –º–æ–¥–µ–ª—å—é DeepSeek (—Ç–æ–ª—å–∫–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ ID)
        deepseek_models = ['deepseek-chat', 'deepseek-reasoner']
        if model in deepseek_models:
            # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á DeepSeek –∏–∑ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
            deepseek_key = api_keys_mgr.get_key('deepseek')
            if not deepseek_key:
                # Fallback –Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
                deepseek_key = os.environ.get('DEEPSEEK_API_KEY')
                if not deepseek_key:
                    # –ü–æ—Å–ª–µ–¥–Ω–∏–π fallback –Ω–∞ OpenAI –∫–ª—é—á
                    try:
                        current_app.logger.warning('DEEPSEEK_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è OPENAI_API_KEY')
                    except Exception:
                        pass
                    deepseek_key = self.api_key
            
            # –°–æ–∑–¥–∞—ë–º –∫–ª–∏–µ–Ω—Ç –¥–ª—è DeepSeek API
            return openai.OpenAI(
                api_key=deepseek_key,
                base_url="https://api.deepseek.com"
            )
        # Perplexity (—Å–µ–º–µ–π—Å—Ç–≤–æ sonar: sonar, sonar-pro, sonar-reasoning, sonar-reasoning-pro, sonar-deep-research)
        pplx_prefixes = ('sonar',)
        if any(model.startswith(p) for p in pplx_prefixes):
            pplx_key = api_keys_mgr.get_key('perplexity') or os.environ.get('PPLX_API_KEY') or os.environ.get('PERPLEXITY_API_KEY') or self.api_key
            return openai.OpenAI(api_key=pplx_key, base_url="https://api.perplexity.ai")
        
        # –î–ª—è –º–æ–¥–µ–ª–µ–π OpenAI –ø–æ–ª—É—á–∞–µ–º –∫–ª—é—á –∏–∑ –º–µ–Ω–µ–¥–∂–µ—Ä–∞
        openai_key = api_keys_mgr.get_key('openai') or self.api_key
        return openai.OpenAI(api_key=openai_key)


def get_rag_service(
    database_url: Optional[str] = None,
    api_key: Optional[str] = None
) -> RAGService:
    """
    –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è RAG-—Å–µ—Ä–≤–∏—Å–∞.
    
    Args:
        database_url: URL –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        api_key: API –∫–ª—é—á OpenAI
        
    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä RAGService
    """
    return RAGService(database_url=database_url, api_key=api_key)
