"""Blueprint –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
import os
import re
import json
import html as htmllib
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, current_app, Response, g
from webapp.services.files import allowed_file
from webapp.services.state import FilesState
from webapp.services.indexing import get_index_path
from webapp.services.db_indexing import build_db_index
from webapp.models.rag_models import RAGDatabase
from webapp.config.config_service import get_config

search_bp = Blueprint('search', __name__)


def _get_db() -> RAGDatabase:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î (–∫–µ—à–∏—Ä—É–µ—Ç—Å—è –≤ g)."""
    if 'db' not in g:
        config = get_config()
        dsn = config.database_url.replace('postgresql+psycopg2://', 'postgresql://')
        g.db = RAGDatabase(dsn)
    return g.db


def _get_current_user_id() -> int:
    """
    –ü–æ–ª—É—á–∏—Ç—å ID —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ —Å–µ—Å—Å–∏–∏/—Ç–æ–∫–µ–Ω–∞.
    
    TODO: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏.
    –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ID –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.
    """
    # –ó–∞–≥–ª—É—à–∫–∞: –≤ –±—É–¥—É—â–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ JWT —Ç–æ–∫–µ–Ω–∞ –∏–ª–∏ —Å–µ—Å—Å–∏–∏
    return g.get('user_id', 1)  # default owner_id=1 –¥–ª—è dev


def _search_in_db(db: RAGDatabase, owner_id: int, keywords: list, exclude_mode: bool = False) -> list:
    """
    –ü–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º –≤ –ë–î —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ owner_id –∏ is_visible.
    
    Args:
        db: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        owner_id: ID –≤–ª–∞–¥–µ–ª—å—Ü–∞
        keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        exclude_mode: –ï—Å–ª–∏ True, –∏—â–µ—Ç —Ñ–∞–π–ª—ã –ë–ï–ó –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    results = []
    
    try:
        with db.db.connect() as conn:
            with conn.cursor() as cur:
                # –§–æ—Ä–º–∏—Ä—É–µ–º tsquery –¥–ª—è –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
                query_terms = ' | '.join(keywords)  # OR –º–µ–∂–¥—É —Ç–µ—Ä–º–∏–Ω–∞–º–∏
                
                if exclude_mode:
                    # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ –ù–ò –û–î–ò–ù —Ç–µ—Ä–º–∏–Ω –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
                    cur.execute("""
                        SELECT DISTINCT d.id, d.original_filename
                        FROM documents d
                        WHERE d.owner_id = %s
                          AND d.is_visible = TRUE
                          AND NOT EXISTS (
                              SELECT 1 FROM chunks c
                              WHERE c.document_id = d.id
                                AND to_tsvector('russian', c.text) @@ to_tsquery('russian', %s)
                          );
                    """, (owner_id, query_terms))
                    
                    rows = cur.fetchall()
                    for row in rows:
                        results.append({
                            'file': row[1],
                            'matches': [],
                            'match_count': 0,
                            'status': 'no_match'
                        })
                else:
                    # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫: –∏—â–µ–º —á–∞–Ω–∫–∏ —Å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏
                    cur.execute("""
                        SELECT 
                            d.id,
                            d.original_filename,
                            c.chunk_idx,
                            c.text,
                            ts_headline('russian', c.text, to_tsquery('russian', %s), 'MaxWords=20, MinWords=10') as snippet
                        FROM documents d
                        JOIN chunks c ON c.document_id = d.id
                        WHERE d.owner_id = %s
                          AND d.is_visible = TRUE
                          AND to_tsvector('russian', c.text) @@ to_tsquery('russian', %s)
                        ORDER BY d.original_filename, c.chunk_idx
                        LIMIT 500;
                    """, (query_terms, owner_id, query_terms))
                    
                    rows = cur.fetchall()
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º
                    file_matches = {}
                    for row in rows:
                        doc_id, filename, chunk_idx, text, snippet = row
                        
                        if filename not in file_matches:
                            file_matches[filename] = {
                                'file': filename,
                                'matches': [],
                                'match_count': 0,
                                'doc_id': doc_id
                            }
                        
                        file_matches[filename]['matches'].append({
                            'chunk_idx': chunk_idx,
                            'snippet': snippet,
                            'text': text[:200]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        })
                        file_matches[filename]['match_count'] += 1
                    
                    results = list(file_matches.values())
                    
    except Exception:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î")
        raise
    
    return results


def _update_document_access_metrics(db: RAGDatabase, results: list) -> None:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (access_count, last_accessed_at).
    
    Args:
        db: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    if not results:
        return
    
    try:
        doc_ids = [r.get('doc_id') for r in results if r.get('doc_id')]
        if not doc_ids:
            return
        
        with db.db.connect() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    UPDATE documents
                    SET access_count = access_count + 1,
                        last_accessed_at = CURRENT_TIMESTAMP
                    WHERE id = ANY(%s);
                """, (doc_ids,))
            conn.commit()
            
        current_app.logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {len(doc_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
    except Exception as e:
        current_app.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: {e}")


    conn.commit()


def _get_files_state():
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä FilesState –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    results_file = current_app.config['SEARCH_RESULTS_FILE']
    return FilesState(results_file)


@search_bp.route('/search', methods=['POST'])
def search():
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è 015).
    
    –ü–æ–∏—Å–∫ –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –ë–î —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ owner_id –∏ is_visible=TRUE.
    Legacy —Ñ–∞–π–ª–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.
    """
    search_terms = request.json.get('search_terms', '')
    exclude_mode = request.json.get('exclude_mode', False)
    
    if not search_terms.strip():
        return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞'}), 400
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ –±–æ–ª–µ–µ 10 —Ç–µ—Ä–º–∏–Ω–æ–≤, –¥–ª–∏–Ω–∞ 2..64, —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    raw_terms = [t.strip() for t in search_terms.split(',') if t.strip()]
    if len(raw_terms) > 50:  # –∂—ë—Å—Ç–∫–∏–π –ø—Ä–µ–¥–µ–ª –Ω–∞ –≤—Ö–æ–¥
        raw_terms = raw_terms[:50]
    filtered = []
    seen = set()
    for t in raw_terms[:10]:
        if 2 <= len(t) <= 64 and t.lower() not in seen:
            seen.add(t.lower())
            filtered.append(t)
    if not filtered:
        return jsonify({'error': '–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ –∏–ª–∏ –ø—É—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'}), 400

    current_app.logger.info(f"–ü–æ–∏—Å–∫ –≤ –ë–î: terms='{','.join(filtered)}', exclude_mode={exclude_mode}")
    
    db = _get_db()
    owner_id = _get_current_user_id()
    
    try:
        # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ owner_id –∏ is_visible=TRUE
        results = _search_in_db(db, owner_id, filtered, exclude_mode)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (access_count, last_accessed_at)
        _update_document_access_metrics(db, results)
        
        current_app.logger.info(f"–ü–æ–∏—Å–∫ –≤ –ë–î –∑–∞–≤–µ—Ä—à—ë–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return jsonify({'results': results})
        
    except Exception as e:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î")
        return jsonify({'error': f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}'}), 500


def _parse_index_groups(index_path: str) -> dict:
    """–ü–∞—Ä—Å–∏—Ç –∏–Ω–¥–µ–∫—Å –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–∞—Ö.
    
    Returns:
        {
            'fast': {'files': 50, 'completed': True, 'size_bytes': 12345},
            'medium': {'files': 30, 'completed': False, 'size_bytes': 0},
            'slow': {'files': 10, 'completed': False, 'size_bytes': 0}
        }
    """
    groups_info = {}
    
    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        for group_name in ['fast', 'medium', 'slow']:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã
            group_pattern = rf'\[–ì–†–£–ü–ü–ê: {group_name.upper()}\].*?–§–∞–π–ª–æ–≤: (\d+).*?–°—Ç–∞—Ç—É—Å: ([^\n]+)'
            match = re.search(group_pattern, content, re.IGNORECASE | re.DOTALL)
            
            if match:
                files_count = int(match.group(1))
                status_text = match.group(2).strip()
                completed = '‚úÖ' in status_text or '–∑–∞–≤–µ—Ä—à–µ–Ω–æ' in status_text.lower()
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≥—Ä—É–ø–ø—ã
                begin_marker = f'<!-- BEGIN_{group_name.upper()} -->'
                end_marker = f'<!-- END_{group_name.upper()} -->'
                group_content = re.search(
                    re.escape(begin_marker) + r'(.*?)' + re.escape(end_marker),
                    content,
                    re.DOTALL
                )
                size_bytes = len(group_content.group(1).strip()) if group_content else 0
                
                groups_info[group_name] = {
                    'files': files_count,
                    'completed': completed,
                    'size_bytes': size_bytes
                }
    except Exception as e:
        current_app.logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥—Ä—É–ø–ø –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    return groups_info


@search_bp.route('/build_index', methods=['POST'])
def build_index_route():
    """–Ø–≤–Ω–∞—è —Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ –ø–∞–ø–∫–µ uploads (—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è 015).
    
    –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –ë–î —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ—Å—Ç—å—é.
    Legacy —Ñ–∞–π–ª–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.
    """
    uploads = current_app.config['UPLOAD_FOLDER']
    if not os.path.exists(uploads):
        return jsonify({'success': False, 'message': '–ü–∞–ø–∫–∞ uploads –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}), 400
    
    config = get_config()
    
    try:
        current_app.logger.info("–ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –ë–î (increment-015)")
        
        db = _get_db()
        owner_id = _get_current_user_id()
        
        success, message, stats = build_db_index(
            db=db,
            owner_id=owner_id,
            folder_path=uploads,
            chunk_size_tokens=config.chunk_size_tokens,
            chunk_overlap_tokens=config.chunk_overlap_tokens
        )
        
        if not success:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –ë–î: {message}")
            return jsonify({'success': False, 'message': message}), 500
        
        current_app.logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ –ë–î –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {message}")
        return jsonify({
            'success': True,
            'message': message,
            'stats': stats
        })
        
    
    except Exception as e:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –∏–Ω–¥–µ–∫—Å–∞")
        return jsonify({'success': False, 'message': str(e)}), 500


@search_bp.get('/index_status')
def index_status():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–∞—Ö.
    
    Returns:
        JSON —Å –ø–æ–ª—è–º–∏:
        - status: idle | running | completed | error
        - group_status: {fast: pending|running|completed, medium: ..., slow: ...}
        - current_group: fast | medium | slow (–µ—Å–ª–∏ running)
        - index_exists: bool
        - index_size: int (–±–∞–π—Ç—ã)
        - groups_info: {fast: {files: int, completed: bool}, ...}
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ progress status —Ñ–∞–π–ª–∞ (increment-013/014)
        index_folder = current_app.config.get('INDEX_FOLDER')
        status_json_path = os.path.join(index_folder, 'status.json') if index_folder else None
        progress_data = None
        
        if status_json_path and os.path.exists(status_json_path):
            try:
                import json
                with open(status_json_path, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
            except Exception as e:
                current_app.logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å status.json: %s", e)
        
        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ uploads –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤, —Å—á–∏—Ç–∞–µ–º –∏–Ω–¥–µ–∫—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º
        uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
        has_files = False
        if os.path.exists(uploads):
            for root, dirs, files in os.walk(uploads):
                for fname in files:
                    if fname == '_search_index.txt' or fname.startswith('~$') or fname.startswith('$'):
                        continue
                    if allowed_file(fname, current_app.config['ALLOWED_EXTENSIONS']):
                        has_files = True
                        break
                if has_files:
                    break
        
        if not has_files:
            response = {'exists': False}
            if progress_data:
                response['progress'] = progress_data
            return jsonify(response)

        # –†–µ–∑–æ–ª–≤–∏–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å (index/ –∏–ª–∏ uploads/)
        idx_primary = get_index_path(current_app.config['INDEX_FOLDER'])
        idx_uploads = os.path.join(uploads, '_search_index.txt')
        idx = idx_primary if os.path.exists(idx_primary) else (idx_uploads if os.path.exists(idx_uploads) else idx_primary)
        exists = os.path.exists(idx)
        
        if not exists:
            response = {'exists': False, 'index_exists': False}
            if progress_data:
                response['progress'] = progress_data
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∏–∑ progress_data –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
                response['status'] = progress_data.get('status', 'idle')
                response['group_status'] = progress_data.get('group_status', {})
                response['current_group'] = progress_data.get('current_group')
            else:
                response['status'] = 'idle'
            return jsonify(response)
        
        size = os.path.getsize(idx)
        mtime = datetime.fromtimestamp(os.path.getmtime(idx)).isoformat()
        
        # –ü–æ–¥—Å—á—ë—Ç –∑–∞–ø–∏—Å–µ–π (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π ===)
        try:
            with open(idx, 'r', encoding='utf-8', errors='ignore') as f:
                entries = sum(1 for line in f if line.strip().startswith('====='))
        except Exception:
            entries = None
        
        response = {
            'exists': True,
            'index_exists': True,
            'index_size': size,
            'size': size,
            'mtime': mtime,
            'entries': entries
        }
        
        # –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—ã –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
        groups_info = _parse_index_groups(idx) if exists else {}
        if groups_info:
            response['groups_info'] = groups_info
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if progress_data:
            response['progress'] = progress_data
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∏–∑ progress_data –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
            prog_status = progress_data.get('status', 'completed')
            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å—Ç–∞—Ç—É—Å –≤ progress_data 'running', –Ω–æ –∏–Ω–¥–µ–∫—Å —É–∂–µ –ø–æ–ª–Ω—ã–π,
            # –∑–Ω–∞—á–∏—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º completed
            if prog_status == 'running' and entries and entries > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º timestamp: –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª—Å—è > 10 —Å–µ–∫—É–Ω–¥, —Å—á–∏—Ç–∞–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º
                try:
                    updated_at = progress_data.get('updated_at')
                    if updated_at:
                        last_update = datetime.fromisoformat(updated_at)
                        if datetime.now() - last_update > timedelta(seconds=10):
                            prog_status = 'completed'
                except Exception:
                    pass
            response['status'] = prog_status
            response['group_status'] = progress_data.get('group_status', {})
            response['current_group'] = progress_data.get('current_group')
        else:
            response['status'] = 'idle'
        
        return jsonify(response)
    
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.get('/view_index')
def view_index():
    """–ü—Ä–æ—Å–º–æ—Ç—Ä —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - raw=1: –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –µ—Å—Ç—å (—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≥—Ä—É–ø–ø)
    - raw=0 (default): –ø–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å—Ç—Ä–æ–∫)
    """
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∂–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å –∏–∑ index/ –∏–ª–∏ uploads/
    uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
    idx_primary = get_index_path(current_app.config['INDEX_FOLDER'])
    idx_uploads = os.path.join(uploads, '_search_index.txt')
    idx = idx_primary if os.path.exists(idx_primary) else (idx_uploads if os.path.exists(idx_uploads) else idx_primary)
    
    try:
        content = None
        if os.path.exists(idx):
            try:
                with open(idx, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            except Exception:
                # –í–æ –≤—Ä–µ–º—è –∞—Ç–æ–º–∞—Ä–Ω–æ–π –∑–∞–º–µ–Ω—ã —Ñ–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                content = None

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–Ω–¥–µ–∫—Å ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–∫–µ–ª–µ—Ç –ø–æ —Å—Ç–∞—Ç—É—Å—É
        if content is None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ç–∞—Ç—É—Å–∞
            progress = None
            status_json_path = os.path.join(current_app.config.get('INDEX_FOLDER'), 'status.json')
            try:
                if status_json_path and os.path.exists(status_json_path):
                    with open(status_json_path, 'r', encoding='utf-8') as sf:
                        progress = json.load(sf)
            except Exception:
                progress = None

            group_labels = {
                'fast': 'TXT, CSV, HTML',
                'medium': 'DOCX, XLSX, –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ PDF',
                'slow': 'PDF-—Å–∫–∞–Ω—ã —Å OCR'
            }
            def map_status(s):
                if s == 'completed':
                    return '‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–æ'
                if s == 'running':
                    return '–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è'
                return '–æ–∂–∏–¥–∞–Ω–∏–µ'
            grp_status = (progress or {}).get('group_status', {}) if progress else {}
            # –°–æ–±–∏—Ä–∞–µ–º —Å–∫–µ–ª–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–∫–∏ + –ø—É—Å—Ç—ã–µ —Å–µ–∫—Ü–∏–∏ –º–µ–∂–¥—É –º–∞—Ä–∫–µ—Ä–∞–º–∏
            parts = []
            for g in ['fast', 'medium', 'slow']:
                parts.append('' )
                parts.append('‚ïê' * 80)
                parts.append(f"[–ì–†–£–ü–ü–ê: {g.upper()}] {group_labels[g]}")
                status_text = map_status(grp_status.get(g)) if grp_status else '–æ–∂–∏–¥–∞–Ω–∏–µ'
                parts.append(f"–§–∞–π–ª–æ–≤: ‚Äî | –°—Ç–∞—Ç—É—Å: {status_text}")
                parts.append('‚ïê' * 80)
                parts.append(f"<!-- BEGIN_{g.upper()} -->")
                parts.append(f"<!-- END_{g.upper()} -->")
                parts.append('')
            content = "\n".join(parts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–æ
        metadata = [
            "# –°–≤–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞",
            f"# –†–∞–∑–º–µ—Ä: {len(content)} –±–∞–π—Ç",
            f"# –û–±–Ω–æ–≤–ª—ë–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "#",
            "# –§–æ—Ä–º–∞—Ç: –≥—Ä—É–ø–ø–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (increment-014)",
            "# –ì—Ä—É–ø–ø—ã: FAST (TXT,CSV,HTML) ‚Üí MEDIUM (DOCX,XLSX,PDF) ‚Üí SLOW (OCR)",
            "#",
            ""
        ]
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø
        groups_info = _parse_index_groups(idx)
        for group_name, info in groups_info.items():
            status = "‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–æ" if info['completed'] else "‚è≥ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è"
            metadata.append(
                f"# {group_name.upper()}: {info['files']} —Ñ–∞–π–ª–æ–≤, "
                f"{info['size_bytes']} –±–∞–π—Ç, {status}"
            )
        
        metadata.append("#\n" + "=" * 80 + "\n")
        
        # –†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∞
        show_raw = request.args.get('raw', '0') == '1'
        q = request.args.get('q') or ''
        
        # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä q, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞
        if not q:
            try:
                files_state = _get_files_state()
                last_terms = files_state.get_last_search_terms()
                if last_terms:
                    q = last_terms
            except Exception:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è
        
        terms = [t.strip() for t in q.split(',') if t and t.strip()]

        if show_raw:
            base_text = '\n'.join(metadata) + '\n' + content
        else:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if line.startswith('‚ïê') or \
                   line.startswith('[–ì–†–£–ü–ü–ê:') or \
                   line.startswith('<!--') or \
                   ('–§–∞–π–ª–æ–≤:' in line and '–°—Ç–∞—Ç—É—Å:' in line):
                    continue
                filtered_lines.append(line)
            base_text = '\n'.join(metadata) + '\n' + '\n'.join(filtered_lines)

        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ ‚Äî –æ—Ç–¥–∞—ë–º –∫–∞–∫ text/plain
        if not terms:
            return Response(base_text, mimetype='text/plain; charset=utf-8')

        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞: —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML, –∑–∞—Ç–µ–º –≤—ã–¥–µ–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è <mark>
        safe = htmllib.escape(base_text)
        highlighted = safe
        for term in terms:
            if not term:
                continue
            try:
                pattern = re.compile(re.escape(term), re.IGNORECASE)
                highlighted = pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", highlighted)
            except re.error:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                continue

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
        q_param = f"&q={htmllib.escape(q)}" if q else ""
        toggle_text = "–ü–æ–∫–∞–∑–∞—Ç—å —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π" if show_raw else "–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"
        toggle_raw = '0' if show_raw else '1'
        
        html_page = (
            "<!DOCTYPE html>\n"
            "<html lang=\"ru\">\n<head>\n<meta charset=\"utf-8\">\n"
            "<title>–°–≤–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî –ø–æ–¥—Å–≤–µ—Ç–∫–∞</title>\n"
            "<style>body{font:14px/1.5 -apple-system,Segoe UI,Arial,sans-serif;padding:16px;}"
            "pre{white-space:pre-wrap;word-wrap:break-word;background:#f8f8f8;padding:12px;border-radius:6px;}"
            "mark{background:#ffeb3b;padding:0 2px;border-radius:2px;}"
            "a.btn{display:inline-block;margin-bottom:12px;text-decoration:none;background:#3498db;color:#fff;padding:6px 10px;border-radius:4px;margin-right:8px;}"
            ".search-info{background:#e8f5e9;padding:8px 12px;border-radius:4px;margin-bottom:12px;display:inline-block;}"
            "</style>\n"
            "</head><body>\n"
            f"<div><a class=\"btn\" href=\"/\">‚Üê –ù–∞ –≥–ª–∞–≤–Ω—É—é</a>"
            f"<a class=\"btn\" href=\"/view_index?raw={toggle_raw}{q_param}\">{toggle_text}</a></div>"
            f"<div class=\"search-info\">üîç –ü–æ–¥—Å–≤–µ—á–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω—ã: <strong>{', '.join(terms)}</strong></div>"
            "<pre>" + highlighted + "</pre>\n"
            "</body></html>\n"
        )
        return Response(html_page, mimetype='text/html; charset=utf-8')
    
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.route('/clear_results', methods=['POST'])
def clear_results():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞."""
    try:
        files_state = _get_files_state()
        files_state.clear()
        current_app.logger.info('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—á–∏—â–µ–Ω—ã')
        return jsonify({'success': True})
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
        return jsonify({'error': str(e)}), 500
