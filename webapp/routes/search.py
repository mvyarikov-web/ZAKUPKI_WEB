"""Blueprint –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
import os
import re
import json
import html as htmllib
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, current_app, Response, g
from webapp.services.files import allowed_file
from webapp.services.state import FilesState
from webapp.services.db_indexing import build_db_index, get_folder_index_status
from webapp.models.rag_models import RAGDatabase
from webapp.config.config_service import get_config

search_bp = Blueprint('search', __name__)


def _get_db() -> RAGDatabase:
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î (–∫–µ—à–∏—Ä—É–µ—Ç—Å—è –≤ g)."""
    if 'db' not in g:
        config = get_config()
        dsn = config.database_url.replace('postgresql+psycopg2://', 'postgresql://')
        g.db = RAGDatabase(dsn)
    return g.db


def required_user_id() -> int:
    """–°—Ç—Ä–æ–≥–æ –ø–æ–ª—É—á–∏—Ç—å user_id (—Å–º. STRICT_USER_ID)."""
    config = get_config()
    strict = config.strict_user_id

    # 1) g.user.id
    try:
        user = getattr(g, 'user', None)
        if user and getattr(user, 'id', None):
            return int(user.id)
    except Exception:
        pass

    # 2) –ó–∞–≥–æ–ª–æ–≤–æ–∫
    try:
        uid = request.headers.get('X-User-ID')
        if uid and str(uid).isdigit():
            return int(uid)
    except Exception:
        pass

    if strict:
        raise ValueError('user_id –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (STRICT_USER_ID)')
    return 1


def _make_snippet(text: str, keywords: list, context_chars: int = 100) -> str:
    """
    –°–æ–∑–¥–∞—ë—Ç —Å–Ω–∏–ø–ø–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤–æ–∫—Ä—É–≥ –ø–µ—Ä–≤–æ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞.
    
    Args:
        text: –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
        context_chars: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ –∏ –ø–æ—Å–ª–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
        
    Returns:
        –°–Ω–∏–ø–ø–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏ –º–Ω–æ–≥–æ—Ç–æ—á–∏—è–º–∏
    """
    text_lower = text.lower()
    
    # –ò—â–µ–º –ø–µ—Ä–≤–æ–µ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ –ª—é–±–æ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
    min_pos = len(text)
    found_keyword = None
    
    for keyword in keywords:
        pos = text_lower.find(keyword.lower())
        if pos != -1 and pos < min_pos:
            min_pos = pos
            found_keyword = keyword
    
    # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—á–∞–ª–æ
    if found_keyword is None:
        return text[:200] + ('...' if len(text) > 200 else '')
    
    # –í—ã—á–∏—Å–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–Ω–∏–ø–ø–µ—Ç–∞
    start = max(0, min_pos - context_chars)
    end = min(len(text), min_pos + len(found_keyword) + context_chars)
    
    snippet = text[start:end]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏—è
    if start > 0:
        snippet = '...' + snippet
    if end < len(text):
        snippet = snippet + '...'
    
    return snippet


def _search_in_db(db: RAGDatabase, owner_id: int, keywords: list, exclude_mode: bool = False) -> list:
    """
    –ü–æ–∏—Å–∫ –ø–æ —á–∞–Ω–∫–∞–º –≤ –ë–î —á–µ—Ä–µ–∑ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ documents —Å –≤–∏–¥–∏–º–æ—Å—Ç—å—é —á–µ—Ä–µ–∑ user_documents.
    
    Args:
        db: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        owner_id: ID –≤–ª–∞–¥–µ–ª—å—Ü–∞
        keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        exclude_mode: –ï—Å–ª–∏ True, –∏—â–µ—Ç —Ñ–∞–π–ª—ã –ë–ï–ó –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        
    Returns:
        –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    results = []
    
    try:
        with db.db.connect() as conn:
            with conn.cursor() as cur:
                if exclude_mode:
                    # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã, –≥–¥–µ –Ω–∏ –æ–¥–∏–Ω —Ç–µ—Ä–º–∏–Ω –Ω–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è
                    conditions = []
                    params = [owner_id]
                    for kw in keywords:
                        conditions.append("c.text NOT ILIKE %s")
                        params.append(f'%{kw}%')
                    where_clause = " AND ".join(conditions)
                    cur.execute(f"""
SELECT DISTINCT d.id, COALESCE(ud.original_filename, d.sha256) AS filename
FROM user_documents ud
JOIN documents d ON d.id = ud.document_id
JOIN chunks c ON c.document_id = d.id
WHERE ud.user_id = %s AND ud.is_soft_deleted = FALSE
  AND {where_clause};
""", params)
                    rows = cur.fetchall()
                    for row in rows:
                        results.append({
                            'file': row[1],
                            'matches': [],
                            'match_count': 0,
                            'status': 'no_match'
                        })
                else:
                    # –û–±—ã—á–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º
                    conditions = []
                    params = [owner_id]
                    for kw in keywords:
                        conditions.append("c.text ILIKE %s")
                        params.append(f'%{kw}%')
                    where_clause = " OR ".join(conditions)
                    cur.execute(f"""
SELECT 
    d.id,
    COALESCE(ud.original_filename, d.sha256) AS filename,
    ud.user_path,
    c.chunk_index,
    c.text
FROM user_documents ud
JOIN documents d ON d.id = ud.document_id
JOIN chunks c ON c.document_id = d.id
WHERE ud.user_id = %s AND ud.is_soft_deleted = FALSE
  AND ({where_clause})
ORDER BY filename, c.chunk_index
LIMIT 500;
""", params)
                    rows = cur.fetchall()
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º –∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Å–æ–±–∏—Ä–∞–µ–º per_term
                    file_matches = {}
                    for row in rows:
                        doc_id, filename, storage_url, chunk_idx, text = row
                        
                        if filename not in file_matches:
                            file_matches[filename] = {
                                'file': filename,
                                'storage_url': storage_url,
                                'filename': filename,   # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –¥–ª—è —Ñ—Ä–æ–Ω—Ç–∞
                                'source': storage_url,  # –∫–ª—é—á –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –≤ UI
                                'path': storage_url,    # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
                                'matches': [],
                                'match_count': 0,
                                'doc_id': doc_id,
                                '_per_term': {t: {'count': 0, 'snippets': []} for t in keywords}
                            }
                        
                        # –°–æ–∑–¥–∞—ë–º —Å–Ω–∏–ø–ø–µ—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤–æ–∫—Ä—É–≥ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                        snippet = _make_snippet(text, keywords, context_chars=100)
                        
                        file_matches[filename]['matches'].append({
                            'chunk_idx': chunk_idx,
                            'snippet': snippet,
                            'text': text[:200]  # –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        })
                        file_matches[filename]['match_count'] += 1

                        # –ü–µ—Ä-—Ç–µ—Ä–º–∏–Ω —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è UI (–ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á—É —Å—á–∏—Ç–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –∏ –±–µ—Ä—ë–º –¥–æ 2 —Å–Ω–∏–ø–ø–µ—Ç–æ–≤)
                        try:
                            lower_text = text.lower()
                            for term in keywords:
                                term_lower = term.lower()
                                # –°—á–∏—Ç–∞–µ–º –≤—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º–∞–º: —Å–ª–æ–≤–æ + –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –±—É–∫–≤/—Ü–∏—Ñ—Ä
                                pattern = re.compile(r"\b" + re.escape(term_lower) + r"\w*\b", re.IGNORECASE)
                                matches = pattern.findall(lower_text)
                                cnt = len(matches)
                                if cnt > 0:
                                    entry = file_matches[filename]['_per_term'][term]
                                    entry['count'] += cnt
                                    # –î–æ –¥–≤—É—Ö —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ –Ω–∞ —Ç–µ—Ä–º–∏–Ω –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
                                    if len(entry['snippets']) < 2:
                                        entry['snippets'].append(_make_snippet(text, [term], context_chars=100))
                        except Exception:
                            # –ù–µ –ª–æ–º–∞–µ–º –æ–±—â–∏–π –ø–æ–∏—Å–∫ –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º –ø–æ–¥—Å—á—ë—Ç–∞
                            pass
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º _per_term –≤ per_term –¥–ª—è –æ—Ç–≤–µ—Ç–∞ (—É–±–∏—Ä–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã —Å 0 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π)
                    results = []
                    for fm in file_matches.values():
                        per_term = []
                        for term, data in fm.get('_per_term', {}).items():
                            count = int(data.get('count', 0))
                            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Ä–º–∏–Ω—ã —Å –Ω—É–ª–µ–≤—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                            if count > 0:
                                per_term.append({
                                    'term': term,
                                    'count': count,
                                    'snippets': data.get('snippets', [])
                                })
                        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞
                        per_term.sort(key=lambda x: x['count'], reverse=True)
                        fm['per_term'] = per_term
                        # –£–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ø–æ–ª–µ
                        fm.pop('_per_term', None)
                        results.append(fm)
                    
    except Exception:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î")
        raise
    
    return results


def _update_document_access_metrics(db: RAGDatabase, results: list) -> None:
    """
    –û–±–Ω–æ–≤–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (access_count, last_accessed_at).
    
    Args:
        db: –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î
        results: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
    """
    if not results:
        return
    
    try:
        doc_ids = [r.get('doc_id') for r in results if r.get('doc_id')]
        if not doc_ids:
            return
        
        with db.db.connect() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    UPDATE documents
                    SET access_count = access_count + 1,
                        last_accessed_at = CURRENT_TIMESTAMP
                    WHERE id = ANY(%s);
                """, (doc_ids,))
            conn.commit()
            
        current_app.logger.debug(f"–û–±–Ω–æ–≤–ª–µ–Ω—ã –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {len(doc_ids)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        
    except Exception as e:
        current_app.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è: {e}")



def _get_files_state():
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä FilesState –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    results_file = current_app.config['SEARCH_RESULTS_FILE']
    return FilesState(results_file)


@search_bp.route('/search', methods=['POST'])
def search():
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º (—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è 015).
    
    –ü–æ–∏—Å–∫ –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –ë–î —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ user_documents (user_id, is_soft_deleted=FALSE).
    Legacy —Ñ–∞–π–ª–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.
    """
    search_terms = request.json.get('search_terms', '')
    exclude_mode = request.json.get('exclude_mode', False)
    
    if not search_terms.strip():
        return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞'}), 400
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ –±–æ–ª–µ–µ 10 —Ç–µ—Ä–º–∏–Ω–æ–≤, –¥–ª–∏–Ω–∞ 2..64, —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    raw_terms = [t.strip() for t in search_terms.split(',') if t.strip()]
    if len(raw_terms) > 50:  # –∂—ë—Å—Ç–∫–∏–π –ø—Ä–µ–¥–µ–ª –Ω–∞ –≤—Ö–æ–¥
        raw_terms = raw_terms[:50]
    filtered = []
    seen = set()
    for t in raw_terms[:10]:
        if 2 <= len(t) <= 64 and t.lower() not in seen:
            seen.add(t.lower())
            filtered.append(t)
    if not filtered:
        return jsonify({'error': '–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ –∏–ª–∏ –ø—É—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'}), 400

    current_app.logger.info(f"–ü–æ–∏—Å–∫ –≤ –ë–î: terms='{','.join(filtered)}', exclude_mode={exclude_mode}")
    
    db = _get_db()
    try:
        owner_id = required_user_id()
    except ValueError:
        return jsonify({'error': '–ù–µ —É–∫–∞–∑–∞–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (X-User-ID)'}), 400
    
    try:
        # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ owner_id –∏ is_visible=TRUE
        results = _search_in_db(db, owner_id, filtered, exclude_mode)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (access_count, last_accessed_at)
        _update_document_access_metrics(db, results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –¥–ª—è UI (/ –∏ /view_index)
        try:
            files_state = _get_files_state()
            files_state.set_last_search_terms(','.join(filtered))
        except Exception:
            current_app.logger.debug('–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã', exc_info=True)
        
        current_app.logger.info(f"–ü–æ–∏—Å–∫ –≤ –ë–î –∑–∞–≤–µ—Ä—à—ë–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return jsonify({'results': results})
        
    except Exception as e:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ë–î")
        return jsonify({'error': f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {str(e)}'}), 500


# –£–¥–∞–ª–µ–Ω–æ: –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (_search_index.txt) ‚Äî legacy


@search_bp.route('/build_index', methods=['POST'])
def build_index_route():
    """–Ø–≤–Ω–∞—è —Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ –ø–∞–ø–∫–µ uploads (—Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è 015).
    
    –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤ –ë–î —Å –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ—Å—Ç—å—é.
    Legacy —Ñ–∞–π–ª–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –±–æ–ª—å—à–µ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è.
    """
    uploads = current_app.config['UPLOAD_FOLDER']
    if not os.path.exists(uploads):
        return jsonify({'success': False, 'message': '–ü–∞–ø–∫–∞ uploads –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}), 400
    
    config = get_config()
    
    try:
        current_app.logger.info("–ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –ë–î (increment-015)")
        
        db = _get_db()
        try:
            owner_id = required_user_id()
        except ValueError:
            return jsonify({'success': False, 'message': '–ù–µ —É–∫–∞–∑–∞–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (X-User-ID)'}), 400
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä force_rebuild –∏–∑ JSON-–∑–∞–ø—Ä–æ—Å–∞
        force_rebuild = request.json.get('force_rebuild', False) if request.is_json else False
        
        success, message, stats = build_db_index(
            db=db,
            owner_id=owner_id,
            folder_path=uploads,
            chunk_size_tokens=config.chunk_size_tokens,
            chunk_overlap_tokens=config.chunk_overlap_tokens,
            force_rebuild=force_rebuild
        )
        
        if not success:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤ –ë–î: {message}")
            return jsonify({'success': False, 'message': message}), 500
        
        current_app.logger.info(f"–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ –ë–î –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {message}")
        return jsonify({
            'success': True,
            'message': message,
            'stats': stats
        })
        
    
    except Exception as e:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –∏–Ω–¥–µ–∫—Å–∞")
        return jsonify({'success': False, 'message': str(e)}), 500


@search_bp.get('/index_status')
def index_status():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–∞—Ö.
    
    Returns:
        JSON —Å –ø–æ–ª—è–º–∏:
        - status: idle | running | completed | error
        - group_status: {fast: pending|running|completed, medium: ..., slow: ...}
        - current_group: fast | medium | slow (–µ—Å–ª–∏ running)
        - index_exists: bool (–Ω–∞–ª–∏—á–∏–µ –≤–∏–¥–∏–º—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è)
        - index_size: int (–±–∞–π—Ç—ã) ‚Äî legacy –ø–æ–ª–µ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–ø—É—â–µ–Ω–æ)
        - groups_info: {fast: {files: int, completed: bool}, ...} ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø –∏–∑ progress.json
    """
    try:
        # 1) –¢–µ–ª–µ–º–µ—Ç—Ä–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ (legacy status.json –¥–ª—è UI)
        index_folder = current_app.config.get('INDEX_FOLDER')
        status_json_path = os.path.join(index_folder, 'status.json') if index_folder else None
        progress_data = None
        if status_json_path and os.path.exists(status_json_path):
            try:
                with open(status_json_path, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
            except Exception as e:
                current_app.logger.debug('–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å status.json: %s', e)

        # 2) –ï—Å—Ç—å –ª–∏ –≤–æ–æ–±—â–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ —Ñ–∞–π–ª—ã –≤ uploads
        uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
        has_files = False
        if os.path.exists(uploads):
            for root, dirs, files in os.walk(uploads):
                for fname in files:
                    if fname == '_search_index.txt' or fname.startswith('~$') or fname.startswith('$'):
                        continue
                    if allowed_file(fname, current_app.config['ALLOWED_EXTENSIONS']):
                        has_files = True
                        break
                if has_files:
                    break

        # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–æ–≤ ‚Äî –∏–Ω–¥–µ–∫—Å–∞ –∫–∞–∫ —Ç–∞–∫–æ–≤–æ–≥–æ –±—ã—Ç—å –Ω–µ –º–æ–∂–µ—Ç
        if not has_files:
            resp = {'exists': False, 'index_exists': False, 'status': 'idle'}
            if progress_data:
                resp['progress'] = progress_data
                # –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å: –æ—Ç–¥–∞—ë–º group_status/group_times, –µ—Å–ª–∏ –µ—Å—Ç—å
                if isinstance(progress_data, dict):
                    if 'group_status' in progress_data:
                        resp['group_status'] = progress_data.get('group_status')
                    if 'group_times' in progress_data:
                        resp['group_times'] = progress_data.get('group_times')
            return jsonify(resp)

        # 3) –°—Ç–∞—Ç—É—Å –∏–∑ –ë–î (increment-015): uploads –∫–∞–∫ tracked folder
        db = _get_db()
        try:
            owner_id = required_user_id()
        except ValueError:
            return jsonify({'error': '–ù–µ —É–∫–∞–∑–∞–Ω –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (X-User-ID)'}), 400
        try:
            db_status = get_folder_index_status(db, owner_id, uploads)
        except Exception:
            current_app.logger.debug('–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—É—Å –∏–∑ –ë–î –¥–ª—è /index_status', exc_info=True)
            db_status = None

        # 4) –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (—á–µ—Ä–µ–∑ user_documents)
        docs_count = 0
        try:
            with db.db.connect() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        SELECT COUNT(*) FROM user_documents
                        WHERE user_id = %s AND is_soft_deleted = FALSE;
                        """,
                        (owner_id,)
                    )
                    row = cur.fetchone()
                    if row and isinstance(row[0], (int,)):
                        docs_count = int(row[0])
        except Exception:
            docs_count = 0

        # 5) –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç (DB-first) —Å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é –ø–æ–ª–µ–π
        response = {
            'exists': docs_count > 0,
            'index_exists': docs_count > 0,  # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å legacy —Ç–µ—Å—Ç–∞–º–∏
            'entries': docs_count,           # map –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –ë–î
            'db': {
                'documents': docs_count,
                'last_indexed_at': (db_status or {}).get('last_indexed_at').isoformat() if (db_status and db_status.get('last_indexed_at')) else None,
                'root_hash': (db_status or {}).get('root_hash')
            }
        }

        # 6) –í–∫–ª—é—á–∞–µ–º —Å—Ç–∞—Ç—É—Å—ã –≥—Ä—É–ø–ø –∏ –≤—Ä–µ–º–µ–Ω–∞, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –≤ progress.json
        if progress_data and isinstance(progress_data, dict):
            response['progress'] = progress_data
            response['status'] = progress_data.get('status', 'completed' if docs_count > 0 else 'idle')
            if 'group_status' in progress_data:
                response['group_status'] = progress_data.get('group_status')
            if 'current_group' in progress_data:
                response['current_group'] = progress_data.get('current_group')
            if 'group_times' in progress_data:
                response['group_times'] = progress_data.get('group_times')
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç progress.json ‚Äî –ø—Ä–æ—Å—Ç–æ –æ—Ç—Ä–∞–∂–∞–µ–º —Ñ–∞–∫—Ç –Ω–∞–ª–∏—á–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            response['status'] = 'completed' if docs_count > 0 else 'idle'

        return jsonify(response)

    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.get('/view_index')
def view_index():
    """–ü—Ä–æ—Å–º–æ—Ç—Ä —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - raw=1: –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –µ—Å—Ç—å (—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≥—Ä—É–ø–ø)
    - raw=0 (default): –ø–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å—Ç—Ä–æ–∫)
    """
    # –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Å–º–æ—Ç—Ä: –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–∫–µ–ª–µ—Ç –ø–æ —Å—Ç–∞—Ç—É—Å—É –≥—Ä—É–ø–ø + —Å–≤–æ–¥–∫—É –ë–î
    uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ç–∞—Ç—É—Å–∞
        progress = None
        status_json_path = os.path.join(current_app.config.get('INDEX_FOLDER'), 'status.json')
        try:
            if status_json_path and os.path.exists(status_json_path):
                with open(status_json_path, 'r', encoding='utf-8') as sf:
                    progress = json.load(sf)
        except Exception:
            progress = None

        group_labels = {
            'fast': 'TXT, CSV, HTML',
            'medium': 'DOCX, XLSX, –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ PDF',
            'slow': 'PDF-—Å–∫–∞–Ω—ã —Å OCR'
        }
        def map_status(s):
            if s == 'completed':
                return '‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–æ'
            if s == 'running':
                return '–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è'
            return '–æ–∂–∏–¥–∞–Ω–∏–µ'
        grp_status = (progress or {}).get('group_status', {}) if progress else {}
        
        # DB-first: —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ë–î
        db = _get_db()
        try:
            owner_id = required_user_id()
        except ValueError:
            # Fallback: –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ–∑–≤–æ–ª—è–µ–º owner_id=1, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª –ø—É—Å—Ç–æ–π —ç–∫—Ä–∞–Ω
            owner_id = 1
        
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ –∏—Ö —á–∞–Ω–∫–∏ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ user_documents
        docs_by_group = {'fast': [], 'medium': [], 'slow': []}
        try:
            with db.db.connect() as conn:
                with conn.cursor() as cur:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏—Ö —á–∞–Ω–∫–∞–º–∏
                    cur.execute("""
                        SELECT 
                            d.id,
                            COALESCE(ud.original_filename, d.sha256) AS filename,
                            ud.user_path,
                            c.chunk_index,
                            c.text
                        FROM user_documents ud
                        JOIN documents d ON d.id = ud.document_id
                        LEFT JOIN chunks c ON c.document_id = d.id
                        WHERE ud.user_id = %s AND ud.is_soft_deleted = FALSE
                        ORDER BY filename, c.chunk_index;
                    """, (owner_id,))
                    rows = cur.fetchall()
                    
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
                    current_doc = None
                    current_chunks = []
                    for row in rows:
                        doc_id, filename, storage_url, chunk_idx, text = row
                        
                        if current_doc is None or current_doc['id'] != doc_id:
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                            if current_doc:
                                current_doc['chunks'] = current_chunks
                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é (—É–ø—Ä–æ—â—ë–Ω–Ω–æ)
                                ext = os.path.splitext(filename)[1].lower()
                                if ext in ['.txt', '.csv', '.html', '.htm']:
                                    docs_by_group['fast'].append(current_doc)
                                elif ext in ['.docx', '.xlsx', '.xls']:
                                    docs_by_group['medium'].append(current_doc)
                                else:
                                    docs_by_group['slow'].append(current_doc)
                            
                            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç
                            current_doc = {
                                'id': doc_id,
                                'filename': filename,
                                'storage_url': storage_url
                            }
                            current_chunks = []
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞–Ω–∫
                        if text:
                            current_chunks.append({
                                'idx': chunk_idx,
                                'text': text,
                                'char_count': len(text)
                            })
                    
                    # –ù–µ –∑–∞–±—ã–≤–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
                    if current_doc:
                        current_doc['chunks'] = current_chunks
                        ext = os.path.splitext(current_doc['filename'])[1].lower()
                        if ext in ['.txt', '.csv', '.html', '.htm']:
                            docs_by_group['fast'].append(current_doc)
                        elif ext in ['.docx', '.xlsx', '.xls']:
                            docs_by_group['medium'].append(current_doc)
                        else:
                            docs_by_group['slow'].append(current_doc)
                            
        except Exception as e:
            current_app.logger.exception('–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ë–î –¥–ª—è view_index')
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –ø—É—Å—Ç—ã–º–∏ –≥—Ä—É–ø–ø–∞–º–∏
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        parts = []
        for g in ['fast', 'medium', 'slow']:
            docs = docs_by_group.get(g, [])
            parts.append('')
            parts.append('‚ïê' * 80)
            parts.append(f"[–ì–†–£–ü–ü–ê: {g.upper()}] {group_labels[g]}")
            status_text = map_status(grp_status.get(g)) if grp_status else '–æ–∂–∏–¥–∞–Ω–∏–µ'
            parts.append(f"–§–∞–π–ª–æ–≤: {len(docs)} | –°—Ç–∞—Ç—É—Å: {status_text}")
            parts.append('‚ïê' * 80)
            parts.append(f"<!-- BEGIN_{g.upper()} -->")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
            for doc in docs:
                parts.append('')
                parts.append(f"__LABEL__–ó–ê–ì–û–õ–û–í–û–ö:__/LABEL__ __HEADER__{doc['filename']}__/HEADER__")
                parts.append(f"__LABEL__–§–æ—Ä–º–∞—Ç:__/LABEL__ {os.path.splitext(doc['filename'])[1]}")
                parts.append(f"__LABEL__–ò—Å—Ç–æ—á–Ω–∏–∫:__/LABEL__ {doc.get('storage_url', 'unknown')}")
                
                # –ü–æ–¥—Å—á—ë—Ç —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ —á–∞–Ω–∫–æ–≤
                total_chars = sum(c.get('char_count', 0) for c in doc.get('chunks', []))
                parts.append(f"__LABEL__–°–∏–º–≤–æ–ª–æ–≤:__/LABEL__ {total_chars}")
                parts.append('')
                
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã —á–∞–Ω–∫–æ–≤
                for chunk in doc.get('chunks', []):
                    if chunk.get('text'):
                        parts.append(chunk['text'])
                        parts.append('')  # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
                
                # 3 –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
                parts.append('')
                parts.append('')
                parts.append('')
            
            parts.append(f"<!-- END_{g.upper()} -->")
            parts.append('')
        
        content = "\n".join(parts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–æ
        metadata = [
            "# –î–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∏–Ω–¥–µ–∫—Å–∞ (DB-first)",
            f"# –û–±–Ω–æ–≤–ª—ë–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "#",
            "# –ì—Ä—É–ø–ø—ã: FAST (TXT,CSV,HTML) ‚Üí MEDIUM (DOCX,XLSX,PDF) ‚Üí SLOW (OCR)",
            "#",
            ""
        ]
        # DB-first —Å–≤–æ–¥–∫–∞ (increment-015): –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å—Ç–∞—Ç—É—Å –ø–∞–ø–∫–∏
        try:
            db = _get_db()
            try:
                owner_id = required_user_id()
            except ValueError:
                owner_id = None
            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–≤–∏–¥–∏–º—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é)
            docs_count = None
            with db.db.connect() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        SELECT COUNT(*) FROM user_documents
                        WHERE user_id = %s AND is_soft_deleted = FALSE;
                        """,
                        (owner_id,)
                    )
                    row = cur.fetchone()
                    docs_count = row[0] if row else None
            # folder_index_status
            db_status = None
            try:
                db_status = get_folder_index_status(db, owner_id, uploads)
            except Exception:
                db_status = None
            last_idx = db_status.get('last_indexed_at').strftime('%Y-%m-%d %H:%M:%S') if (db_status and db_status.get('last_indexed_at')) else '‚Äî'
            root_hash = db_status.get('root_hash') if db_status else '‚Äî'
            metadata.extend([
                "# –ë–î-—Å–≤–æ–¥–∫–∞:",
                f"# –î–æ–∫—É–º–µ–Ω—Ç–æ–≤: {docs_count if docs_count is not None else '‚Äî'}",
                f"# –ü–æ—Å–ª–µ–¥–Ω—è—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {last_idx}",
                f"# Root hash (uploads): {root_hash}",
                "#",
                ""
            ])
        except Exception:
            # –ù–µ –º–µ—à–∞–µ–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—é –ø—Ä–∏ —Å–±–æ–µ –ë–î
            pass
        
        # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –ø–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø –¥–æ—Å—Ç—É–ø–Ω–∞ —á–µ—Ä–µ–∑ /index_status
        
        metadata.append("#\n" + "=" * 80 + "\n")
        
        # –†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∞
        show_raw = request.args.get('raw', '0') == '1'
        q = request.args.get('q') or ''
        
        # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä q, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞
        # –ë–æ–ª—å—à–µ –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ last_search_terms ‚Äî –ø–æ–¥—Å–≤–µ—Ç–∫–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —è–≤–Ω–æ –ø–µ—Ä–µ–¥–∞–Ω q
        
        terms = [t.strip() for t in q.split(',') if t and t.strip()]



        if show_raw:
            base_text = '\n'.join(metadata) + '\n' + content
        else:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if line.startswith('‚ïê') or \
                   line.startswith('[–ì–†–£–ü–ü–ê:') or \
                   line.startswith('<!--') or \
                   ('–§–∞–π–ª–æ–≤:' in line and '–°—Ç–∞—Ç—É—Å:' in line):
                    continue
                filtered_lines.append(line)
            base_text = '\n'.join(metadata) + '\n' + '\n'.join(filtered_lines)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤: –≤—Å–µ–≥–¥–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º __LABEL__/__HEADER__ –≤ HTML
        safe = base_text
        placeholders = {}
        counter = 0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∫–∏ —á–µ—Ä–µ–∑ –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã (–¥–æ —ç–∫—Ä–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è HTML)
        for match in re.finditer(r"__LABEL__(.*?)__/LABEL__", safe):
            placeholder = f"__PH_LABEL_{counter}__"
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –º–µ—Ç–∫–∏
            content = htmllib.escape(match.group(1))
            placeholders[placeholder] = f'<span class="index-document-label">{content}</span>'
            safe = safe.replace(match.group(0), placeholder, 1)
            counter += 1
        
        for match in re.finditer(r"__HEADER__(.*?)__/HEADER__", safe):
            placeholder = f"__PH_HEADER_{counter}__"
            # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            content = htmllib.escape(match.group(1))
            placeholders[placeholder] = f'<span class="index-document-header">{content}</span>'
            safe = safe.replace(match.group(0), placeholder, 1)
            counter += 1
        
        # –≠–∫—Ä–∞–Ω–∏—Ä—É–µ–º –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ
        safe = htmllib.escape(safe)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã —Å —É–∂–µ –≥–æ—Ç–æ–≤—ã–º–∏ span-—Ç–µ–≥–∞–º–∏
        for placeholder, original in placeholders.items():
            safe = safe.replace(placeholder, original)
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ–∏—Å–∫–∞ ‚Äî –æ—Ç–¥–∞—ë–º –±–µ–∑ –ø–æ–¥—Å–≤–µ—Ç–∫–∏
        if not terms:
            html_page = (
                "<!DOCTYPE html>\n"
                "<html lang=\"ru\">\n<head>\n<meta charset=\"utf-8\">\n"
                "<title>–ò–Ω–¥–µ–∫—Å (–ë–î)</title>\n"
                "<style>body{font:14px/1.5 -apple-system,Segoe UI,Arial,sans-serif;padding:16px;}"
                "pre{white-space:pre-wrap;word-wrap:break-word;background:#f8f8f8;padding:12px;border-radius:6px;}"
                ".index-document-header{color:#2196F3 !important;font-weight:bold;}"
                ".index-document-label{color:#1976D2 !important;font-weight:600;}"
                "a.btn{display:inline-block;margin-bottom:12px;text-decoration:none;background:#3498db;color:#fff;padding:6px 10px;border-radius:4px;margin-right:8px;}"
                "</style>\n"
                "</head><body>\n"
                f"<div><a class=\"btn\" href=\"/\">‚Üê –ù–∞ –≥–ª–∞–≤–Ω—É—é</a></div>"
                "<pre>" + safe + "</pre>\n"
                "</body></html>\n"
            )
            return Response(html_page, mimetype='text/html; charset=utf-8')
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ–¥—Å–≤–µ—Ç–∫—É –¥–ª—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –ø–æ–∏—Å–∫–∞
        highlighted = safe
        for term in terms:
            if not term:
                continue
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤ \b –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–µ–ª—ã—Ö —Å–ª–æ–≤ –∏ –∏—Ö —Ñ–æ—Ä–º
                # re.escape –∑–∞—â–∏—â–∞–µ—Ç –æ—Ç —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤, –∞ \b –∏—â–µ—Ç –≥—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤
                pattern = re.compile(r'\b' + re.escape(term) + r'\w*\b', re.IGNORECASE)
                highlighted = pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", highlighted)
            except re.error:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                continue

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
        from urllib.parse import quote
        q_param = f"&q={quote(q)}" if q else ""
        toggle_text = "–ü–æ–∫–∞–∑–∞—Ç—å —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π" if show_raw else "–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"
        toggle_raw = '0' if show_raw else '1'
        
        html_page = (
            "<!DOCTYPE html>\n"
            "<html lang=\"ru\">\n<head>\n<meta charset=\"utf-8\">\n"
            "<title>–ò–Ω–¥–µ–∫—Å (–ë–î) ‚Äî –ø–æ–¥—Å–≤–µ—Ç–∫–∞</title>\n"
            "<style>body{font:14px/1.5 -apple-system,Segoe UI,Arial,sans-serif;padding:16px;}"
            "pre{white-space:pre-wrap;word-wrap:break-word;background:#f8f8f8;padding:12px;border-radius:6px;}"
            "mark{background:#ffeb3b;padding:0 2px;border-radius:2px;}"
            ".index-document-header{color:#2196F3 !important;font-weight:bold;}"
            ".index-document-label{color:#1976D2 !important;font-weight:600;}"
            "a.btn{display:inline-block;margin-bottom:12px;text-decoration:none;background:#3498db;color:#fff;padding:6px 10px;border-radius:4px;margin-right:8px;}"
            ".search-info{background:#e8f5e9;padding:8px 12px;border-radius:4px;margin-bottom:12px;display:inline-block;}"
            "</style>\n"
            "</head><body>\n"
            f"<div><a class=\"btn\" href=\"/\">‚Üê –ù–∞ –≥–ª–∞–≤–Ω—É—é</a>"
            f"<a class=\"btn\" href=\"/view_index?raw={toggle_raw}{q_param}\">{toggle_text}</a></div>"
            f"<div class=\"search-info\">üîç –ü–æ–¥—Å–≤–µ—á–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω—ã: <strong>{', '.join(terms)}</strong></div>"
            "<pre>" + highlighted + "</pre>\n"
            "</body></html>\n"
        )
        return Response(html_page, mimetype='text/html; charset=utf-8')
    
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.route('/clear_results', methods=['POST'])
def clear_results():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞."""
    try:
        files_state = _get_files_state()
        files_state.clear()
        current_app.logger.info('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—á–∏—â–µ–Ω—ã')
        return jsonify({'success': True})
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
        return jsonify({'error': str(e)}), 500
