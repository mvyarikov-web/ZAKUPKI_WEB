"""Blueprint –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
import os
import re
import shutil
import json
import html as htmllib
from datetime import datetime, timedelta
from flask import Blueprint, request, jsonify, current_app, Response
from document_processor import DocumentProcessor
from document_processor.search.searcher import Searcher
from webapp.services.files import allowed_file
from webapp.services.state import FilesState
from webapp.services.indexing import (
    build_search_index, 
    search_in_index, 
    get_index_path,
    parse_index_char_counts
)
from webapp.services.data_access_adapter import DataAccessAdapter

search_bp = Blueprint('search', __name__)


def _get_adapter():
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä DataAccessAdapter –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    return DataAccessAdapter(current_app.config)


def _get_files_state():
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä FilesState –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è."""
    results_file = current_app.config['SEARCH_RESULTS_FILE']
    return FilesState(results_file)


def _search_in_files(search_terms, exclude_mode=False):
    """–ù–û–í–ê–Ø –ª–æ–≥–∏–∫–∞: –ø–æ–∏—Å–∫ –ø–æ —Å–≤–æ–¥–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É (_search_index.txt), –∏–≥–Ω–æ—Ä–∏—Ä—É—è –∑–∞–≥–æ–ª–æ–≤–∫–∏.
    –ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ñ–∞–π–ª–∞–º –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç—É—Å—ã file_status.
    –ü–∞—Ä–∞–º–µ—Ç—Ä exclude_mode: –µ—Å–ª–∏ True, –∏—â–µ—Ç —Ñ–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞.
    """
    results = []
    terms = [t.strip() for t in search_terms.split(',') if t.strip()]
    if not terms:
        return results

    uploads = current_app.config['UPLOAD_FOLDER']
    if not os.path.exists(uploads):
        current_app.logger.warning('–ü–∞–ø–∫–∞ uploads –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ø—Ä–∏ –ø–æ–∏—Å–∫–µ')
        return results

    # –°–æ–±–∏—Ä–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤
    files_to_search = []
    for root, dirs, files in os.walk(uploads):
        for fname in files:
            # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ Office-—Ñ–∞–π–ª—ã
            if fname == '_search_index.txt' or fname.startswith('~$') or fname.startswith('$'):
                continue
            if allowed_file(fname, current_app.config['ALLOWED_EXTENSIONS']):
                rel_path = os.path.relpath(os.path.join(root, fname), uploads)
                files_to_search.append(rel_path)

    # –†–µ–∑–æ–ª–≤–µ—Ä –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞: index/_search_index.txt –∏–ª–∏ uploads/_search_index.txt
    def _resolve_live_index_path():
        idx_in_index = get_index_path(current_app.config['INDEX_FOLDER'])
        if os.path.exists(idx_in_index):
            return idx_in_index
        idx_in_uploads = os.path.join(uploads, '_search_index.txt')
        if os.path.exists(idx_in_uploads):
            return idx_in_uploads
        return None

    # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (–µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –∑–∞–ø—É—Å—Ç–∏–º —Å–±–æ—Ä–∫—É –≤ —Ñ–æ–Ω–µ –∏ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ–º UI)
    index_path = _resolve_live_index_path()
    if not index_path:
        current_app.logger.info('–ò–Ω–¥–µ–∫—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî –∑–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é —Å–±–æ—Ä–∫—É (use_groups=True) –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç')
        try:
            import threading
            def _bg_build():
                try:
                    adapter = _get_adapter()
                    success, message, char_counts = adapter.build_index(use_groups=True)
                    if success:
                        current_app.logger.info(f'–§–æ–Ω–æ–≤–∞—è —Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {message}')
                    else:
                        current_app.logger.error(f'–û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {message}')
                except Exception:
                    current_app.logger.exception('–û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏')
            threading.Thread(target=_bg_build, daemon=True).start()
        except Exception:
            current_app.logger.exception('–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ç–∞—Ä—Ç–æ–≤–∞—Ç—å —Ñ–æ–Ω–æ–≤—É—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é')
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ: UI –ø–æ–∫–∞–∂–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –≥—Ä—É–ø–ø –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç –ø–æ–≤—Ç–æ—Ä–∏—Ç—å –ø–æ–∏—Å–∫ —Å–ø—É—Å—Ç—è —Å–µ–∫—É–Ω–¥—ã
        return results

    # –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É —Å –ø—Ä–∏–≤—è–∑–∫–æ–π –∫ —Ñ–∞–π–ª–∞–º —á–µ—Ä–µ–∑ DataAccessAdapter
    try:
        adapter = _get_adapter()
        current_app.logger.info(f"–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É: terms={terms}, exclude_mode={exclude_mode}")
        matches = adapter.search_documents(
            keywords=terms,
            user_id=None,  # TODO: –ø–æ–ª—É—á–∞—Ç—å –∏–∑ request.user –ø–æ—Å–ª–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
            exclude_mode=exclude_mode,
            context_chars=80
        )
    except Exception as e:
        current_app.logger.exception(f'–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É: {e}')
        matches = []

    # –°–≥—Ä—É–ø–ø–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ñ–∞–π–ª–∞–º –∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –≤–Ω—É—Ç—Ä–∏ —Ñ–∞–π–ª–∞
    grouped: dict[str, dict] = {}
    for m in matches:
        title = m.get('title') or m.get('source') or '–∏–Ω–¥–µ–∫—Å'
        kw = m.get('keyword', '')
        snip = m.get('snippet', '')
        is_exclude = m.get('exclude_mode', False)
        g = grouped.setdefault(title, { 'by_term': {}, 'total': 0, 'exclude_mode': is_exclude })
        tinfo = g['by_term'].setdefault(kw, { 'count': 0, 'snippets': [] })
        tinfo['count'] += 1
        g['total'] += 1
        # –í —Ä–µ–∂–∏–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è - —Ç–æ–ª—å–∫–æ 1 —Å–Ω–∏–ø–ø–µ—Ç
        max_snippets = 1 if is_exclude else 3
        if len(tinfo['snippets']) < max_snippets and snip:
            tinfo['snippets'].append(snip)

    # –û–±–Ω–æ–≤–∏–º —Å—Ç–∞—Ç—É—Å—ã –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤—ã–¥–∞—á—É
    files_state = _get_files_state()
    # –ü–æ–ª—É—á–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –∫–æ–ª–∏—á–µ—Å—Ç–≤ —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ (—Ä–µ–∞–ª—å–Ω—ã–µ –∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏)
    try:
        counts = parse_index_char_counts(index_path)
    except Exception:
        counts = {}
    found_files = set()
    new_statuses = {}
    
    for rel_path, data in grouped.items():
        # –°–æ—Å—Ç–∞–≤–∏–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ –¥–æ 3 —Å–Ω–∏–ø–ø–µ—Ç–æ–≤ –Ω–∞ —Ç–µ—Ä–º–∏–Ω
        found_terms = []
        context = []
        is_exclude = data.get('exclude_mode', False)
        
        if is_exclude:
            # –í —Ä–µ–∂–∏–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç" –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            for term in terms:
                found_terms.append(f"–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç: {term}")
            # –í —Ä–µ–∂–∏–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è —Ç–æ–ª—å–∫–æ 1 —Å–Ω–∏–ø–ø–µ—Ç
            for term, info in data['by_term'].items():
                context.extend(info['snippets'][:1])
        else:
            for term, info in data['by_term'].items():
                if not term:
                    continue
                found_terms.append(f"{term} ({info['count']})")
                context.extend(info['snippets'][:3])
        
        new_entry = {
            'status': 'contains_keywords',
            'found_terms': found_terms,
            'context': context[: max(3, len(context))],
            'processed_at': datetime.now().isoformat()
        }
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–π —Å—Ç–∞—Ç—É—Å (–¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö –∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤)
        prev = files_state.get_file_status(rel_path)
        # –ù–µ –∑–∞—Ç–∏—Ä–∞–µ–º —Ä–∞–Ω–µ–µ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ –ø–æ–ª—è (char_count, error, original_name)
        for k in ('char_count','error','original_name'):
            if k in prev:
                new_entry[k] = prev[k]
        # –û–±–Ω–æ–≤–ª—è–µ–º char_count –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏
        if isinstance(rel_path, str) and rel_path in counts:
            new_entry['char_count'] = counts[rel_path]
        
        # –í —Ä–µ–∂–∏–º–µ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞: –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
        # (—Ñ–∞–π–ª—ã —Å char_count == 0 –∏–ª–∏ —Å –æ—à–∏–±–∫–∞–º–∏)
        if is_exclude:
            char_count = new_entry.get('char_count', 0)
            if char_count == 0:
                current_app.logger.debug(f"–ü—Ä–æ–ø—É—Å–∫ –Ω–µ–ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –≤ —Ä–µ–∂–∏–º–µ exclude: {rel_path}")
                continue
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–ª—è –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–≤–∫–ª—é—á–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤)
        new_statuses[rel_path] = new_entry
        found_files.add(rel_path)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤—ã–¥–∞—á—É
        # –§–æ—Ä–º–∏—Ä—É–µ–º –±–ª–æ–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É
        per_term = []
        if is_exclude:
            # –í —Ä–µ–∂–∏–º–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è: –æ–¥–∏–Ω –±–ª–æ–∫ –¥–ª—è –≤—Å–µ—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º "–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç"
            all_snippets = []
            for term_data in data['by_term'].values():
                all_snippets.extend(term_data['snippets'][:1])
            
            for original_term in terms:
                per_term.append({
                    'term': f'–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç: {original_term}',
                    'count': 1,
                    'snippets': all_snippets[:1]  # –¢–æ–ª—å–∫–æ 1 —Å–Ω–∏–ø–ø–µ—Ç
                })
        else:
            for term, info in data['by_term'].items():
                per_term.append({
                    'term': term,
                    'count': info['count'],
                    'snippets': info['snippets']
                })
        results.append({
            'filename': os.path.basename(rel_path) if isinstance(rel_path, str) else str(rel_path),
            'source': rel_path,
            'path': rel_path if rel_path in files_to_search else None,
            'total': data['total'],
            'per_term': per_term
        })

    # –†–µ–∞–ª—å–Ω—ã–º —Ñ–∞–π–ª–∞–º –±–µ–∑ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π ‚Äî —Å—Ç–∞—Ç—É—Å "–Ω–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤"
    for rel_path in files_to_search:
        if rel_path not in found_files:
            prev = files_state.get_file_status(rel_path)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º char_count - –µ—Å–ª–∏ 0, —Ç–æ —Ñ–∞–π–ª –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å
            char_count = prev.get('char_count', 0) if isinstance(prev, dict) else 0
            if char_count == 0:
                new_entry = {
                    'status': 'error',
                    'processed_at': datetime.now().isoformat()
                }
            else:
                new_entry = {
                    'status': 'no_keywords',
                    'processed_at': datetime.now().isoformat()
                }
            for k in ('char_count','error','original_name'):
                if k in prev:
                    new_entry[k] = prev[k]
            # –ï—Å–ª–∏ –µ—Å—Ç—å —Å—á—ë—Ç—á–∏–∫ —Å–∏–º–≤–æ–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å–µ ‚Äî —Å–æ—Ö—Ä–∞–Ω–∏–º –µ–≥–æ
            if rel_path in counts:
                new_entry['char_count'] = counts[rel_path]
            new_statuses[rel_path] = new_entry

    # –ê—Ç–æ–º–∞—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö —Å—Ç–∞—Ç—É—Å–æ–≤
    if new_statuses:
        files_state.update_file_statuses(new_statuses)

    current_app.logger.info(f"–ü–æ–∏—Å–∫ –∑–∞–≤–µ—Ä—à—ë–Ω: –Ω–∞–π–¥–µ–Ω–æ {len(matches)} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π, –≥—Ä—É–ø–ø: {len(grouped)}")
    return results


@search_bp.route('/search', methods=['POST'])
def search():
    """–ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    search_terms = request.json.get('search_terms', '')
    exclude_mode = request.json.get('exclude_mode', False)
    
    if not search_terms.strip():
        return jsonify({'error': '–í–≤–µ–¥–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞'}), 400
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è: –Ω–µ –±–æ–ª–µ–µ 10 —Ç–µ—Ä–º–∏–Ω–æ–≤, –¥–ª–∏–Ω–∞ 2..64, —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
    raw_terms = [t.strip() for t in search_terms.split(',') if t.strip()]
    if len(raw_terms) > 50:  # –∂—ë—Å—Ç–∫–∏–π –ø—Ä–µ–¥–µ–ª –Ω–∞ –≤—Ö–æ–¥
        raw_terms = raw_terms[:50]
    filtered = []
    seen = set()
    for t in raw_terms[:10]:
        if 2 <= len(t) <= 64 and t.lower() not in seen:
            seen.add(t.lower())
            filtered.append(t)
    if not filtered:
        return jsonify({'error': '–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ –∏–ª–∏ –ø—É—Å—Ç—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞'}), 400

    # –ù–æ–≤—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å
    current_app.logger.info(f"–ó–∞–ø—Ä–æ—Å –ø–æ–∏—Å–∫–∞: terms='{','.join(filtered)}' (–∏–∑ {len(raw_terms)} –≤—Ö–æ–¥–Ω—ã—Ö), exclude_mode={exclude_mode}")
    results = _search_in_files(','.join(filtered), exclude_mode=exclude_mode)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ (—Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã)
    files_state = _get_files_state()
    files_state.set_last_search_terms(','.join(filtered))
    
    return jsonify({'results': results})


def _parse_index_groups(index_path: str) -> dict:
    """–ü–∞—Ä—Å–∏—Ç –∏–Ω–¥–µ–∫—Å –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–∞—Ö.
    
    Returns:
        {
            'fast': {'files': 50, 'completed': True, 'size_bytes': 12345},
            'medium': {'files': 30, 'completed': False, 'size_bytes': 0},
            'slow': {'files': 10, 'completed': False, 'size_bytes': 0}
        }
    """
    groups_info = {}
    
    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        for group_name in ['fast', 'medium', 'slow']:
            # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≥—Ä—É–ø–ø—ã
            group_pattern = rf'\[–ì–†–£–ü–ü–ê: {group_name.upper()}\].*?–§–∞–π–ª–æ–≤: (\d+).*?–°—Ç–∞—Ç—É—Å: ([^\n]+)'
            match = re.search(group_pattern, content, re.IGNORECASE | re.DOTALL)
            
            if match:
                files_count = int(match.group(1))
                status_text = match.group(2).strip()
                completed = '‚úÖ' in status_text or '–∑–∞–≤–µ—Ä—à–µ–Ω–æ' in status_text.lower()
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –≥—Ä—É–ø–ø—ã
                begin_marker = f'<!-- BEGIN_{group_name.upper()} -->'
                end_marker = f'<!-- END_{group_name.upper()} -->'
                group_content = re.search(
                    re.escape(begin_marker) + r'(.*?)' + re.escape(end_marker),
                    content,
                    re.DOTALL
                )
                size_bytes = len(group_content.group(1).strip()) if group_content else 0
                
                groups_info[group_name] = {
                    'files': files_count,
                    'completed': completed,
                    'size_bytes': size_bytes
                }
    except Exception as e:
        current_app.logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≥—Ä—É–ø–ø –∏–Ω–¥–µ–∫—Å–∞: {e}")
    
    return groups_info


@search_bp.route('/build_index', methods=['POST'])
def build_index_route():
    """–Ø–≤–Ω–∞—è —Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –ø–æ –ø–∞–ø–∫–µ uploads."""
    uploads = current_app.config['UPLOAD_FOLDER']
    if not os.path.exists(uploads):
        return jsonify({'success': False, 'message': '–ü–∞–ø–∫–∞ uploads –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}), 400
    
    # –ì—Ä—É–ø–ø–æ–≤–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∞–∫—Ç–∏–≤–Ω–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (increment-014)
    use_groups = request.json.get('use_groups', True) if request.is_json else True
    
    try:
        adapter = _get_adapter()
        current_app.logger.info(f"–ó–∞–ø—É—Å–∫ —è–≤–Ω–æ–π —Å–±–æ—Ä–∫–∏ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è uploads (use_groups={use_groups})")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DataAccessAdapter –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞
        success, message, char_counts = adapter.build_index(use_groups=use_groups)
        
        if not success:
            current_app.logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞: {message}")
            return jsonify({'success': False, 'message': message}), 500
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ –∏–Ω–¥–µ–∫—Å—É –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        index_path = get_index_path(current_app.config['INDEX_FOLDER'])
        size = os.path.getsize(index_path) if os.path.exists(index_path) else 0
        current_app.logger.info(f"–ò–Ω–¥–µ–∫—Å —Å–æ–±—Ä–∞–Ω: {index_path}, —Ä–∞–∑–º–µ—Ä: {size} –±–∞–π—Ç, —Ñ–∞–π–ª–æ–≤: {len(char_counts)}")
        
        # –û–±–Ω–æ–≤–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É (–≤–∫–ª—é—á–∞—è –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤) –∏ —Å—Ç–∞—Ç—É—Å—ã –æ—à–∏–±–æ–∫/–Ω–µ–ø–æ–¥–¥–µ—Ä–∂–∫–∏
        try:
            # char_counts —É–∂–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ adapter.build_index()
            counts = char_counts
            
            # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ä–µ–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ uploads
            all_files: list[str] = []
            for root, dirs, files in os.walk(uploads):
                for fname in files:
                    if fname == '_search_index.txt' or fname.startswith('~$') or fname.startswith('$'):
                        continue
                    rel_path = os.path.relpath(os.path.join(root, fname), uploads)
                    all_files.append(rel_path)
            
            files_state = _get_files_state()
            new_statuses = {}
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã
            for rel_path in all_files:
                ext_ok = allowed_file(rel_path, current_app.config['ALLOWED_EXTENSIONS'])
                entry = files_state.get_file_status(rel_path).copy()
                
                if not ext_ok:
                    # –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç
                    entry.update({
                        'status': 'unsupported',
                        'error': '–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç',
                        'char_count': 0,
                        'processed_at': datetime.now().isoformat()
                    })
                else:
                    cc = counts.get(rel_path)
                    if cc is None:
                        # –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π, –Ω–æ –Ω–µ—Ç –∑–∞–ø–∏—Å–∏ –≤ –∏–Ω–¥–µ–∫—Å–µ ‚Äî –æ—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è/–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
                        entry.update({
                            'status': entry.get('status', 'error' if entry.get('status') in (None, 'not_checked') else entry.get('status')),
                            'error': entry.get('error') or '–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–ª–∏ –Ω–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω',
                            'char_count': 0,
                            'processed_at': datetime.now().isoformat()
                        })
                    else:
                        # –ï—Å—Ç—å —Å—á—ë—Ç—á–∏–∫ —Å–∏–º–≤–æ–ª–æ–≤ ‚Äî –Ω–µ —Ç—Ä–æ–≥–∞–µ–º —Å—Ç–∞—Ç—É—Å –ø–æ–∏—Å–∫–∞, —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–æ–π
                        entry.update({
                            'char_count': cc,
                            'processed_at': datetime.now().isoformat()
                        })
                        # –µ—Å–ª–∏ 0 —Å–∏–º–≤–æ–ª–æ–≤, –æ—Å—Ç–∞–≤–∏–º —ç—Ç–æ –∫–∞–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –∫–∞—á–µ—Å—Ç–≤–∞ (UI –ø–æ–¥—Å–≤–µ—Ç–∏—Ç)
                
                new_statuses[rel_path] = entry
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤ (—Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ –∏–Ω–¥–µ–∫—Å–µ, –Ω–æ –Ω–µ –≤ all_files)
            for indexed_path, char_count in counts.items():
                if indexed_path not in all_files and '://' in indexed_path:
                    # –≠—Ç–æ –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –∏–∑ –∞—Ä—Ö–∏–≤–∞
                    entry = files_state.get_file_status(indexed_path).copy()
                    entry.update({
                        'char_count': char_count,
                        'processed_at': datetime.now().isoformat()
                    })
                    # –ï—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤ not_checked
                    if not entry.get('status'):
                        entry['status'] = 'not_checked'
                    new_statuses[indexed_path] = entry
            
            # –ê—Ç–æ–º–∞—Ä–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Å—Ç–∞—Ç—É—Å—ã
            files_state.update_file_statuses(new_statuses)
        except Exception:
            current_app.logger.exception('–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å char_count –ø–æ –∏–Ω–¥–µ–∫—Å—É')
        
        return jsonify({'success': True, 'index_path': index_path, 'size': size})
    
    except Exception as e:
        current_app.logger.exception("–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–±–æ—Ä–∫–µ –∏–Ω–¥–µ–∫—Å–∞")
        return jsonify({'success': False, 'message': str(e)}), 500


@search_bp.get('/index_status')
def index_status():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –≥—Ä—É–ø–ø–∞—Ö.
    
    Returns:
        JSON —Å –ø–æ–ª—è–º–∏:
        - status: idle | running | completed | error
        - group_status: {fast: pending|running|completed, medium: ..., slow: ...}
        - current_group: fast | medium | slow (–µ—Å–ª–∏ running)
        - index_exists: bool
        - index_size: int (–±–∞–π—Ç—ã)
        - groups_info: {fast: {files: int, completed: bool}, ...}
    """
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ progress status —Ñ–∞–π–ª–∞ (increment-013/014)
        index_folder = current_app.config.get('INDEX_FOLDER')
        status_json_path = os.path.join(index_folder, 'status.json') if index_folder else None
        progress_data = None
        
        if status_json_path and os.path.exists(status_json_path):
            try:
                import json
                with open(status_json_path, 'r', encoding='utf-8') as f:
                    progress_data = json.load(f)
            except Exception as e:
                current_app.logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å status.json: %s", e)
        
        # –ï—Å–ª–∏ –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ uploads –Ω–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤, —Å—á–∏—Ç–∞–µ–º –∏–Ω–¥–µ–∫—Å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–º
        uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
        has_files = False
        if os.path.exists(uploads):
            for root, dirs, files in os.walk(uploads):
                for fname in files:
                    if fname == '_search_index.txt' or fname.startswith('~$') or fname.startswith('$'):
                        continue
                    if allowed_file(fname, current_app.config['ALLOWED_EXTENSIONS']):
                        has_files = True
                        break
                if has_files:
                    break
        
        if not has_files:
            response = {'exists': False}
            if progress_data:
                response['progress'] = progress_data
            return jsonify(response)

        # –†–µ–∑–æ–ª–≤–∏–º –∞–∫—Ç—É–∞–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å (index/ –∏–ª–∏ uploads/)
        idx_primary = get_index_path(current_app.config['INDEX_FOLDER'])
        idx_uploads = os.path.join(uploads, '_search_index.txt')
        idx = idx_primary if os.path.exists(idx_primary) else (idx_uploads if os.path.exists(idx_uploads) else idx_primary)
        exists = os.path.exists(idx)
        
        if not exists:
            response = {'exists': False, 'index_exists': False}
            if progress_data:
                response['progress'] = progress_data
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∏–∑ progress_data –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
                response['status'] = progress_data.get('status', 'idle')
                response['group_status'] = progress_data.get('group_status', {})
                response['current_group'] = progress_data.get('current_group')
            else:
                response['status'] = 'idle'
            return jsonify(response)
        
        size = os.path.getsize(idx)
        mtime = datetime.fromtimestamp(os.path.getmtime(idx)).isoformat()
        
        # –ü–æ–¥—Å—á—ë—Ç –∑–∞–ø–∏—Å–µ–π (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π ===)
        try:
            with open(idx, 'r', encoding='utf-8', errors='ignore') as f:
                entries = sum(1 for line in f if line.strip().startswith('====='))
        except Exception:
            entries = None
        
        response = {
            'exists': True,
            'index_exists': True,
            'index_size': size,
            'size': size,
            'mtime': mtime,
            'entries': entries
        }
        
        # –ü–∞—Ä—Å–∏–º –≥—Ä—É–ø–ø—ã –∏–∑ –∏–Ω–¥–µ–∫—Å–∞
        groups_info = _parse_index_groups(idx) if exists else {}
        if groups_info:
            response['groups_info'] = groups_info
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞
        if progress_data:
            response['progress'] = progress_data
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –∏–∑ progress_data –Ω–∞ –≤–µ—Ä—Ö–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å
            prog_status = progress_data.get('status', 'completed')
            # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ —Å—Ç–∞—Ç—É—Å –≤ progress_data 'running', –Ω–æ –∏–Ω–¥–µ–∫—Å —É–∂–µ –ø–æ–ª–Ω—ã–π,
            # –∑–Ω–∞—á–∏—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–∏–ª–∞—Å—å ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º completed
            if prog_status == 'running' and entries and entries > 0:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º timestamp: –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª—Å—è > 10 —Å–µ–∫—É–Ω–¥, —Å—á–∏—Ç–∞–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º
                try:
                    updated_at = progress_data.get('updated_at')
                    if updated_at:
                        last_update = datetime.fromisoformat(updated_at)
                        if datetime.now() - last_update > timedelta(seconds=10):
                            prog_status = 'completed'
                except Exception:
                    pass
            response['status'] = prog_status
            response['group_status'] = progress_data.get('group_status', {})
            response['current_group'] = progress_data.get('current_group')
        else:
            response['status'] = 'idle'
        
        return jsonify(response)
    
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.get('/view_index')
def view_index():
    """–ü—Ä–æ—Å–º–æ—Ç—Ä —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –∞–≤—Ç–æ–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º.
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç query-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:
    - raw=1: –ø–æ–∫–∞–∑–∞—Ç—å –∏–Ω–¥–µ–∫—Å –∫–∞–∫ –µ—Å—Ç—å (—Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –≥—Ä—É–ø–ø)
    - raw=0 (default): –ø–æ–∫–∞–∑–∞—Ç—å —Ç–æ–ª—å–∫–æ –∑–∞–ø–∏—Å–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–±–µ–∑ —Å–ª—É–∂–µ–±–Ω—ã—Ö —Å—Ç—Ä–æ–∫)
    """
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∂–∏–≤–æ–π –∏–Ω–¥–µ–∫—Å –∏–∑ index/ –∏–ª–∏ uploads/
    uploads = current_app.config.get('UPLOAD_FOLDER', 'uploads')
    idx_primary = get_index_path(current_app.config['INDEX_FOLDER'])
    idx_uploads = os.path.join(uploads, '_search_index.txt')
    idx = idx_primary if os.path.exists(idx_primary) else (idx_uploads if os.path.exists(idx_uploads) else idx_primary)
    
    try:
        content = None
        if os.path.exists(idx):
            try:
                with open(idx, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
            except Exception:
                # –í–æ –≤—Ä–µ–º—è –∞—Ç–æ–º–∞—Ä–Ω–æ–π –∑–∞–º–µ–Ω—ã —Ñ–∞–π–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                content = None

        # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–Ω–¥–µ–∫—Å ‚Äî —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Å–∫–µ–ª–µ—Ç –ø–æ —Å—Ç–∞—Ç—É—Å—É
        if content is None:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å —Å—Ç–∞—Ç—É—Å–∞
            progress = None
            status_json_path = os.path.join(current_app.config.get('INDEX_FOLDER'), 'status.json')
            try:
                if status_json_path and os.path.exists(status_json_path):
                    with open(status_json_path, 'r', encoding='utf-8') as sf:
                        progress = json.load(sf)
            except Exception:
                progress = None

            group_labels = {
                'fast': 'TXT, CSV, HTML',
                'medium': 'DOCX, XLSX, –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ PDF',
                'slow': 'PDF-—Å–∫–∞–Ω—ã —Å OCR'
            }
            def map_status(s):
                if s == 'completed':
                    return '‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–æ'
                if s == 'running':
                    return '–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è'
                return '–æ–∂–∏–¥–∞–Ω–∏–µ'
            grp_status = (progress or {}).get('group_status', {}) if progress else {}
            # –°–æ–±–∏—Ä–∞–µ–º —Å–∫–µ–ª–µ—Ç: –∑–∞–≥–æ–ª–æ–≤–∫–∏ + –ø—É—Å—Ç—ã–µ —Å–µ–∫—Ü–∏–∏ –º–µ–∂–¥—É –º–∞—Ä–∫–µ—Ä–∞–º–∏
            parts = []
            for g in ['fast', 'medium', 'slow']:
                parts.append('' )
                parts.append('‚ïê' * 80)
                parts.append(f"[–ì–†–£–ü–ü–ê: {g.upper()}] {group_labels[g]}")
                status_text = map_status(grp_status.get(g)) if grp_status else '–æ–∂–∏–¥–∞–Ω–∏–µ'
                parts.append(f"–§–∞–π–ª–æ–≤: ‚Äî | –°—Ç–∞—Ç—É—Å: {status_text}")
                parts.append('‚ïê' * 80)
                parts.append(f"<!-- BEGIN_{g.upper()} -->")
                parts.append(f"<!-- END_{g.upper()} -->")
                parts.append('')
            content = "\n".join(parts)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –Ω–∞—á–∞–ª–æ
        metadata = [
            f"# –°–≤–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å –ø–æ–∏—Å–∫–∞",
            f"# –†–∞–∑–º–µ—Ä: {len(content)} –±–∞–π—Ç",
            f"# –û–±–Ω–æ–≤–ª—ë–Ω: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"#",
            f"# –§–æ—Ä–º–∞—Ç: –≥—Ä—É–ø–ø–æ–≤–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (increment-014)",
            f"# –ì—Ä—É–ø–ø—ã: FAST (TXT,CSV,HTML) ‚Üí MEDIUM (DOCX,XLSX,PDF) ‚Üí SLOW (OCR)",
            f"#",
            ""
        ]
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –≥—Ä—É–ø–ø
        groups_info = _parse_index_groups(idx)
        for group_name, info in groups_info.items():
            status = "‚úÖ –∑–∞–≤–µ—Ä—à–µ–Ω–æ" if info['completed'] else "‚è≥ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è"
            metadata.append(
                f"# {group_name.upper()}: {info['files']} —Ñ–∞–π–ª–æ–≤, "
                f"{info['size_bytes']} –±–∞–π—Ç, {status}"
            )
        
        metadata.append("#\n" + "=" * 80 + "\n")
        
        # –†–µ–∂–∏–º –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –ø–æ–¥—Å–≤–µ—Ç–∫–∞
        show_raw = request.args.get('raw', '0') == '1'
        q = request.args.get('q') or ''
        
        # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω –ø–∞—Ä–∞–º–µ—Ç—Ä q, –ø—ã—Ç–∞–µ–º—Å—è –≤–∑—è—Ç—å —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–æ–∏—Å–∫–∞
        if not q:
            try:
                files_state = _get_files_state()
                last_terms = files_state.get_last_search_terms()
                if last_terms:
                    q = last_terms
            except Exception:
                pass  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏ —á—Ç–µ–Ω–∏—è
        
        terms = [t.strip() for t in q.split(',') if t and t.strip()]

        if show_raw:
            base_text = '\n'.join(metadata) + '\n' + content
        else:
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏
            lines = content.split('\n')
            filtered_lines = []
            for line in lines:
                if line.startswith('‚ïê') or \
                   line.startswith('[–ì–†–£–ü–ü–ê:') or \
                   line.startswith('<!--') or \
                   ('–§–∞–π–ª–æ–≤:' in line and '–°—Ç–∞—Ç—É—Å:' in line):
                    continue
                filtered_lines.append(line)
            base_text = '\n'.join(metadata) + '\n' + '\n'.join(filtered_lines)

        # –ï—Å–ª–∏ –Ω–µ—Ç —Ç–µ—Ä–º–∏–Ω–æ–≤ ‚Äî –æ—Ç–¥–∞—ë–º –∫–∞–∫ text/plain
        if not terms:
            return Response(base_text, mimetype='text/plain; charset=utf-8')

        # –ü–æ–¥—Å–≤–µ—Ç–∫–∞: —ç–∫—Ä–∞–Ω–∏—Ä—É–µ–º HTML, –∑–∞—Ç–µ–º –≤—ã–¥–µ–ª—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è <mark>
        safe = htmllib.escape(base_text)
        highlighted = safe
        for term in terms:
            if not term:
                continue
            try:
                pattern = re.compile(re.escape(term), re.IGNORECASE)
                highlighted = pattern.sub(lambda m: f"<mark>{m.group(0)}</mark>", highlighted)
            except re.error:
                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                continue

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∫–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–∞
        q_param = f"&q={htmllib.escape(q)}" if q else ""
        toggle_text = "–ü–æ–∫–∞–∑–∞—Ç—å —Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π" if show_raw else "–ü–æ–∫–∞–∑–∞—Ç—å –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É"
        toggle_raw = '0' if show_raw else '1'
        
        html_page = (
            "<!DOCTYPE html>\n"
            "<html lang=\"ru\">\n<head>\n<meta charset=\"utf-8\">\n"
            "<title>–°–≤–æ–¥–Ω—ã–π –∏–Ω–¥–µ–∫—Å ‚Äî –ø–æ–¥—Å–≤–µ—Ç–∫–∞</title>\n"
            "<style>body{font:14px/1.5 -apple-system,Segoe UI,Arial,sans-serif;padding:16px;}"
            "pre{white-space:pre-wrap;word-wrap:break-word;background:#f8f8f8;padding:12px;border-radius:6px;}"
            "mark{background:#ffeb3b;padding:0 2px;border-radius:2px;}"
            "a.btn{display:inline-block;margin-bottom:12px;text-decoration:none;background:#3498db;color:#fff;padding:6px 10px;border-radius:4px;margin-right:8px;}"
            ".search-info{background:#e8f5e9;padding:8px 12px;border-radius:4px;margin-bottom:12px;display:inline-block;}"
            "</style>\n"
            "</head><body>\n"
            f"<div><a class=\"btn\" href=\"/\">‚Üê –ù–∞ –≥–ª–∞–≤–Ω—É—é</a>"
            f"<a class=\"btn\" href=\"/view_index?raw={toggle_raw}{q_param}\">{toggle_text}</a></div>"
            f"<div class=\"search-info\">üîç –ü–æ–¥—Å–≤–µ—á–µ–Ω—ã —Ç–µ—Ä–º–∏–Ω—ã: <strong>{', '.join(terms)}</strong></div>"
            "<pre>" + highlighted + "</pre>\n"
            "</body></html>\n"
        )
        return Response(html_page, mimetype='text/html; charset=utf-8')
    
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –∏–Ω–¥–µ–∫—Å–∞')
        return jsonify({'error': str(e)}), 500


@search_bp.route('/clear_results', methods=['POST'])
def clear_results():
    """–û—á–∏—Å—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞."""
    try:
        files_state = _get_files_state()
        files_state.clear()
        current_app.logger.info('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –æ—á–∏—â–µ–Ω—ã')
        return jsonify({'success': True})
    except Exception as e:
        current_app.logger.exception('–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤')
        return jsonify({'error': str(e)}), 500
