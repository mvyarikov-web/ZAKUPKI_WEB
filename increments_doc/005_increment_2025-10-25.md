# Приращение 005: Реализация RAG-анализа документов закупок

**Дата:** 2025-10-25  
**Автор:** GitHub Copilot  
**Статус:** Завершено

## Постановка

### Цель инкремента
Внедрить RAG-систему (Retrieval-Augmented Generation) для углубленного анализа документов закупок климатической техники. Обеспечить ответы на основе документов (а не «из головы» модели) с минимальными затратами токенов и управлением стоимости.

### Основные требования
1. **Индексация документов**: векторное представление с использованием эмбеддингов OpenAI
2. **Семантический поиск**: поиск релевантных фрагментов по векторному сходству
3. **Структурированные ответы**: summary, equipment[], installation с evidence
4. **Управление стоимостью**: оценка до запроса, фактический расход после
5. **Гибкая конфигурация**: выбор модели, Top-K, ручной ввод цен

## Спецификация

### Архитектура RAG-модуля

```
┌─────────────────┐
│   Web UI        │ ← Модальные окна, формы, отображение результатов
└────────┬────────┘
         │
┌────────▼────────┐
│ Flask Routes    │ ← /ai_rag/* endpoints
│ (ai_rag_bp)     │
└────────┬────────┘
         │
┌────────▼────────┐
│  RAG Service    │ ← Основная логика индексации и поиска
└────┬─────┬──────┘
     │     │
     │     └──────┐
     │            │
┌────▼─────┐ ┌───▼──────┐ ┌──────────┐
│Chunking  │ │Embeddings│ │PostgreSQL│
│Service   │ │Service   │ │+ pgvector│
└──────────┘ └──────────┘ └──────────┘
```

### Компоненты

#### 1. База данных (PostgreSQL + pgvector)
- **Таблица `documents`**: метаданные файлов (путь, имя, хеш, размер)
- **Таблица `chunks`**: текстовые фрагменты с векторными эмбеддингами (1536 измерений)
- **Индексы**: IVFFlat для быстрого векторного поиска

#### 2. Сервисы

**webapp/services/chunking.py**
- Разбиение текста на фрагменты 1500-3000 токенов
- Overlap 2-3 предложения для контекста
- Учёт семантических границ (заголовки, разделы)
- Очистка текста от мусора

**webapp/services/embeddings.py**
- Генерация векторных представлений через OpenAI
- Модель: `text-embedding-3-small` (1536 размерность)
- Батч-обработка для эффективности

**webapp/services/rag_service.py**
- Индексация документов
- Векторный поиск релевантных чанков
- Формирование контекста для LLM
- Структурированная генерация ответов
- Постобработка (дедупликация, агрегация)

#### 3. API эндпоинты (webapp/routes/ai_rag.py)
- `GET /ai_rag/models` — список моделей
- `POST /ai_rag/models` — обновление цен
- `POST /ai_rag/estimate` — оценка запроса
- `POST /ai_rag/index` — индексация документов
- `POST /ai_rag/analyze` — выполнение RAG-анализа
- `GET /ai_rag/status` — статус системы

#### 4. UI (templates/index.html, static/js/rag-analysis.js)
- Кнопки: "Анализ RAG", "Индексировать", "Статус"
- Модальное окно промпта с настройками (Top-K, макс. токены)
- Модальное окно выбора модели с ручным вводом цен
- Блок оценки запроса (входные/выходные токены, стоимость)
- Модальное окно результатов с форматированием
- Блок фактического расхода после анализа

#### 5. Конфигурация (webapp/config.py)
```python
RAG_ENABLED = True
RAG_CHUNK_SIZE = 2000  # токены
RAG_CHUNK_OVERLAP = 3  # предложения
RAG_TOP_K = 5
RAG_MIN_SIMILARITY = 0.7
RAG_EMBEDDING_MODEL = 'text-embedding-3-small'
RAG_DEFAULT_MODEL = 'gpt-4o-mini'
RAG_MAX_OUTPUT_TOKENS = 600
DATABASE_URL = 'postgresql://user:pass@localhost:5432/zakupki_rag'
```

## Реализация (простыми словами)

### Что сделано

1. **Создана база данных для RAG**
   - Таблицы для хранения документов и их векторных представлений
   - Поддержка векторного поиска через расширение pgvector
   - Индексы для быстрого поиска по косинусной близости

2. **Реализовано чанкование текста**
   - Разбиение длинных документов на смысловые фрагменты
   - Overlap для сохранения контекста
   - Очистка текста от мусора (лишние пробелы, дефисы переноса)
   - Извлечение разделов документа

3. **Подключены эмбеддинги OpenAI**
   - Генерация векторов для текстовых фрагментов
   - Батч-обработка для экономии времени
   - Graceful degradation при ошибках API

4. **Создан основной RAG-сервис**
   - Индексация документов: извлечение текста → чанкование → эмбеддинги → сохранение в БД
   - Поиск: эмбеддинг запроса → векторный поиск → Top-K релевантных чанков
   - Генерация: сборка контекста → промпт → GPT → структурированный ответ
   - Постобработка: дедупликация оборудования, агрегация вердикта монтажа

5. **Добавлены API эндпоинты**
   - Управление моделями и ценами
   - Оценка запроса до выполнения
   - Индексация выбранных файлов
   - Выполнение RAG-анализа
   - Проверка статуса системы

6. **Создан UI для RAG**
   - Кнопки в главном интерфейсе
   - Модальное окно настройки запроса
   - Модальное окно выбора модели с ценами
   - Отображение оценки и фактического расхода
   - Форматированный вывод результатов (summary, equipment, installation)

7. **Написаны тесты**
   - 7 тестов для чанкования (все проходят)
   - Тесты для эмбеддингов (требуют API ключ)
   - Интеграционные тесты для БД (требуют PostgreSQL)

8. **Создан скрипт инициализации**
   - `scripts/init_rag_db.py` для настройки PostgreSQL
   - Автоматическое создание таблиц и расширений

### Как работает RAG-анализ

1. **Пользователь выбирает файлы** и нажимает "Индексировать"
   - Система извлекает текст из документов
   - Разбивает на чанки по 2000 токенов
   - Получает эмбеддинги для каждого чанка
   - Сохраняет в PostgreSQL с векторами

2. **Пользователь открывает модальное окно "Анализ RAG"**
   - Вводит промпт (редактируемый)
   - Выбирает модель через кнопку "Модель"
   - Настраивает Top-K (3-10 фрагментов) и макс. токены
   - Вводит цены для расчёта стоимости

3. **Нажимает "Оценить"**
   - Система примерно рассчитывает:
     - Входные токены: системный промпт + промпт пользователя + Top-K чанков
     - Выходные токены: по заданному значению
     - Стоимость: по введённым ценам

4. **Нажимает "Начать анализ"**
   - Система получает эмбеддинг промпта
   - Ищет Top-K релевантных чанков по векторному сходству
   - Формирует контекст из найденных фрагментов
   - Отправляет запрос к GPT с требованием JSON-ответа
   - Парсит структурированный ответ
   - Дедуплицирует оборудование, агрегирует вердикт монтажа

5. **Получает результат**
   - **Summary**: краткая выжимка до 10 пунктов
   - **Equipment**: список оборудования с характеристиками и цитатами
   - **Installation**: вердикт (true/false/unknown) + подтверждающие цитаты
   - **Usage**: фактический расход токенов и стоимость

### Пример структурированного ответа

```json
{
  "summary": [
    "Закупка кондиционеров для офисных помещений",
    "Требуется 10 единиц оборудования мощностью 3.5 кВт",
    "В стоимость входит монтаж и пусконаладка"
  ],
  "equipment": [
    {
      "name": "Кондиционер настенный",
      "model": "Daikin FTXS35K",
      "characteristics": {
        "мощность": "3.5 кВт",
        "производительность": "300 м³/ч",
        "класс энергоэффективности": "A+"
      },
      "qty": 10,
      "unit": "шт",
      "evidence": [
        "Кондиционеры настенные Daikin FTXS35K, мощность 3.5 кВт, 10 штук"
      ]
    }
  ],
  "installation": {
    "verdict": true,
    "evidence": [
      "Монтаж и пусконаладка входят в стоимость",
      "Требуется шеф-монтаж силами Подрядчика"
    ]
  }
}
```

### Управление стоимостью

1. **До запроса**: оценка на основе Top-K и промпта
   - Примерный подсчёт токенов (системный промпт + промпт + чанки)
   - Расчёт стоимости по введённым ценам
   - Информирование пользователя (без блокировки)

2. **После запроса**: фактический расход
   - Реальное количество токенов из `response.usage`
   - Расчёт фактической стоимости
   - Отображение в модальном окне результатов

3. **Ручной ввод цен**
   - Пользователь вводит цены для выбранной модели
   - Цены сохраняются в `index/models.json`
   - Используются для всех расчётов
   - По умолчанию 0.0 (без блокировки)

## Примечания

### Требования к окружению
- **PostgreSQL 12+** с расширением `pgvector`
- **OpenAI API ключ** в переменной `OPENAI_API_KEY`
- **Python 3.9+** с зависимостями из `requirements.txt`

### Установка PostgreSQL и pgvector

**Ubuntu/Debian:**
```bash
sudo apt-get install postgresql-16 postgresql-16-pgvector
```

**macOS:**
```bash
brew install postgresql pgvector
```

**Docker:**
```bash
docker run -d \
  -e POSTGRES_PASSWORD=postgres \
  -p 5432:5432 \
  ankane/pgvector
```

### Инициализация базы данных

1. Создать базу данных:
```bash
createdb zakupki_rag
```

2. Установить переменную окружения:
```bash
export DATABASE_URL='postgresql://postgres:password@localhost:5432/zakupki_rag'
```

3. Запустить скрипт инициализации:
```bash
python scripts/init_rag_db.py
```

### Graceful degradation

RAG-модуль корректно работает при отсутствии PostgreSQL:
- Кнопка "Статус" покажет, что БД недоступна
- API вернёт 503 с понятным сообщением
- Основной функционал (классический поиск, AI-анализ) продолжит работать

### Ограничения

1. **Размер контекста**: определяется моделью (128K для gpt-4o-mini)
2. **Top-K фрагментов**: 3-10 (больше — дороже, меньше — хуже качество)
3. **Стоимость**: зависит от модели и объёма документов
4. **Язык**: промпты и результаты на русском, имена переменных на английском

### Производительность

- **Индексация**: ~10-30 сек на документ (зависит от размера)
- **Поиск**: ~1-2 сек (векторный поиск по индексу)
- **Анализ**: ~10-30 сек (зависит от модели и Top-K)

### Безопасность

- API ключ хранится в переменных окружения (не в коде)
- Пароль БД не логируется
- Проверка путей к файлам (защита от traversal)
- Валидация входных данных на всех уровнях

## Тестирование

### Запуск тестов чанкования
```bash
cd /home/runner/work/ZAKUPKI_WEB/ZAKUPKI_WEB
python -m pytest tests/test_rag_module.py::TestChunking -v
```

**Результат:** ✓ 7/7 тестов проходят

### Интеграционные тесты (требуют настройку)

**С PostgreSQL:**
```bash
export DATABASE_URL='postgresql://postgres:password@localhost:5432/zakupki_rag'
python -m pytest tests/test_rag_module.py::TestRAGIntegration -v
```

**С OpenAI API:**
```bash
export OPENAI_API_KEY='sk-...'
python -m pytest tests/test_rag_module.py::TestEmbeddings::test_get_embedding_real_api -v
```

## Файлы изменений

### Новые файлы
- `webapp/models/__init__.py`
- `webapp/models/rag_models.py` — модели БД
- `webapp/services/chunking.py` — чанкование текста
- `webapp/services/embeddings.py` — эмбеддинги OpenAI
- `webapp/services/rag_service.py` — основной RAG-сервис
- `webapp/routes/ai_rag.py` — API эндпоинты
- `static/js/rag-analysis.js` — клиентская логика
- `scripts/init_rag_db.py` — инициализация БД
- `tests/test_rag_module.py` — тесты
- `index/models.json` — конфигурация моделей

### Изменённые файлы
- `requirements.txt` — добавлены: psycopg2-binary, pgvector, openai, tiktoken
- `webapp/config.py` — добавлены RAG-параметры
- `webapp/__init__.py` — зарегистрирован ai_rag_bp
- `templates/index.html` — добавлена секция RAG и модальные окна

## Итоги

### Достигнуто
- ✅ Полноценная RAG-система с векторным поиском
- ✅ Структурированные ответы (summary, equipment, installation)
- ✅ Управление стоимостью через UI
- ✅ Graceful degradation при отсутствии PostgreSQL
- ✅ Русская локализация всего интерфейса
- ✅ 7 тестов для базовой функциональности
- ✅ Документация по настройке

### Не вошло в scope (осознанно)
- Автопарсинг цен моделей из интернета
- Полная нормализация единиц измерения
- OCR на уровне RAG (используется существующая индексация)
- Сложные табличные распознавания
- BM25/гибридный поиск (инфраструктура готова, реализация опциональна)

### Возможные улучшения (для будущих инкрементов)
- Реализация BM25 для гибридного поиска
- Кэширование эмбеддингов по хешу чанка
- Переиспользование существующих эмбеддингов при изменении документа
- Экспорт результатов в JSON/Excel
- История RAG-запросов
- Сравнение результатов разных моделей
- Тонкая настройка температуры и других параметров LLM
